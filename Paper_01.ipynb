{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_QSlsNoFtwkm",
        "cbDeRI_665k3",
        "ETgB4s0O3RNa",
        "FyW9tTBr29-h",
        "eQdw_d183F_9",
        "PbQAuBjO3Len"
      ],
      "authorship_tag": "ABX9TyO75PzlrVPyOXOHJ74Q20JN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehrangEbrahimi13/Repo_Paper_01/blob/%2302-Implement/Paper_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "lH7r2TioTLMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Null Generation for a Dataset"
      ],
      "metadata": {
        "id": "cVX43SalTOlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_random_array(shape, low, high, round, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=(shape)).round(round)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp"
      ],
      "metadata": {
        "id": "YziyJ6QCTRaC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1 : PMIC\n",
        "Generally, feature selection does not work on missing data, so imputation is needed beforehand. However, considering irrelevant features severely affect imputation, we use MIC to select features on missing data by ‘‘partial sample strategy’’ (PSS), which is called **PMIC**. ‘‘Partial sample strategy’’ means using the available values of all feature variables and class variable to calculate MIC"
      ],
      "metadata": {
        "id": "fLdXDn2-Iz-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HuUmBs5TWXZ"
      },
      "outputs": [],
      "source": [
        "!pip install minepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from minepy import MINE\n",
        "\n",
        "mine = MINE()\n",
        "def pmic_feature_selection(F, C, m):\n",
        "    num_features = F.shape[1]\n",
        "    pmic_scores = np.zeros(num_features)\n",
        "\n",
        "    for i in range(num_features):\n",
        "        musk = ~np.isnan(F[:, i])\n",
        "        # Select column i without null values\n",
        "        feature_without_null = F[musk, i]\n",
        "\n",
        "        # Filter y based on non-null values in column i\n",
        "        class_without_null = C[musk]\n",
        "\n",
        "        # Calculate the MIC (Maximal Information Coefficient) score for the current feature and class\n",
        "        mine.compute_score(feature_without_null, class_without_null)\n",
        "        pmic_scores[i] = mine.mic()\n",
        "\n",
        "    # Sort the indices of pmic_scores in descending order and select the top m indices\n",
        "    top_m_features_idx = np.argsort(pmic_scores)[::-1][:m]\n",
        "    return top_m_features_idx"
      ],
      "metadata": {
        "id": "TGdIK_9NJzcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 1 with Functions"
      ],
      "metadata": {
        "id": "KaGBfJPWrGVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 48\n",
        "percentage = 0.5\n",
        "select_top = 3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=0, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "# Generate a random 5x1 array with one-digit random values (0-9)\n",
        "Y = generate_random_array((row,), 0, 10, 0, seed=48, as_dataframe=False)\n",
        "print(\"\\ny: \\n\", Y)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "selected_idx = pmic_feature_selection(X_with_null, Y, select_top)\n",
        "selected_features = X_with_null[:, selected_idx]\n",
        "\n",
        "print(\"\\nX_with_null: \\n\", X_with_null)\n",
        "print(f\"\\nSelected {select_top} indices of features ({percentage * 100} percent null):\\n\", selected_idx)\n",
        "print(f\"\\nSelected {select_top} features ({percentage * 100} percent null):\\n\", selected_features)"
      ],
      "metadata": {
        "id": "A3gBqNfbih7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 : Imputation for the missing data"
      ],
      "metadata": {
        "id": "aqh3Unrhrexe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-negative Latent Factor\n",
        "\n",
        "Description:\n",
        "*   R: Incomplete data matrix of shape (n, m).\n",
        "*   d: Rank of the non-negative latent factors.\n",
        "*   lambda1, lambda2: Regularization parameters.\n",
        "*   max_iter: Maximum number of iterations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_QSlsNoFtwkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter):\n",
        "    # Initialize non-negative matrix P randomly\n",
        "    n, m = R.shape\n",
        "    R_copy = np.copy(R)\n",
        "    P = np.random.rand(n, d)\n",
        "\n",
        "    # Initialize non-negative matrix Q randomly\n",
        "    Q = np.random.rand(d, m)\n",
        "\n",
        "    # Initialize I according to (17)\n",
        "    I = np.ones((n, m))\n",
        "    R_nan_mask = np.isnan(R_copy)\n",
        "    I[R_nan_mask] = 0\n",
        "\n",
        "    # Set zero for Null\n",
        "    R_copy[R_nan_mask] = 0\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    iter = 0\n",
        "\n",
        "    # Convergence criterion\n",
        "    converge = False\n",
        "\n",
        "    while not converge and iter < max_iter:\n",
        "        # Update P according to (22)\n",
        "        P_new = P * ((I * R_copy) @ Q.T) / ((I * (P @ Q)) @ Q.T + lambda1 * P)\n",
        "\n",
        "        # Update Q according to (23)\n",
        "        Q_new = Q * (P_new.T @ (I * R_copy)) / (P_new.T @ (I * (P_new @ Q)) + lambda2 * Q)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.allclose(P, P_new) and np.allclose(Q, Q_new):\n",
        "            converge = True\n",
        "\n",
        "        # Update P and Q\n",
        "        P = P_new\n",
        "        Q = Q_new\n",
        "\n",
        "        # Increment iteration counter\n",
        "        iter += 1\n",
        "\n",
        "    # Impute R by (11) and obtain R_cpl\n",
        "    PQ = np.round( P @ Q, decimals=3)\n",
        "    R_cpl = np.where(R_nan_mask, PQ, R)\n",
        "\n",
        "    return R_cpl\n"
      ],
      "metadata": {
        "id": "gMXdVUfztlRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperimpute"
      ],
      "metadata": {
        "id": "cbDeRI_665k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperimpute"
      ],
      "metadata": {
        "id": "JGuFnGw4664W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hyperimpute.plugins.imputers import Imputers\n",
        "imputers = Imputers()\n",
        "\n",
        "X = pd.DataFrame([[1, 4, 7, 10], [4, 7, np.nan, np.nan], [3, 6, 9, 12], [8, 11, 14, 17]])\n",
        "\n",
        "method = \"gain\"\n",
        "\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(X.copy()).round(2)\n",
        "\n",
        "print(method, out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Dcu9uw7OUP",
        "outputId": "61580f22-99c2-400d-e8b7-66c8c1130de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gain      0     1     2      3\n",
            "0  1.0   4.0   7.0  10.00\n",
            "1  4.0   7.0  10.3  13.45\n",
            "2  3.0   6.0   9.0  12.00\n",
            "3  8.0  11.0  14.0  17.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 2 Non-negative Latent Factor and GAIN with Functions"
      ],
      "metadata": {
        "id": "ETgB4s0O3RNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 48\n",
        "round = 2\n",
        "percentage = 0.3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=round, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "print(\"\\nX_with_null : \\n\", X_with_null)\n",
        "\n",
        "R = np.copy(X_with_null)\n",
        "d = 2\n",
        "lambda1 = 0.1\n",
        "lambda2 = 0.2\n",
        "max_iter = 100\n",
        "\n",
        "# Code execution\n",
        "R_cpl = Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter)\n",
        "print(\"\\nComplete data after imputation by non_negative_latent_factor: \\n\", R_cpl)\n",
        "\n",
        "\n",
        "method = \"gain\"\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(R.copy()).round(round)\n",
        "\n",
        "print(f'\\nComplete data after imputation by {method}: \\n', out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9A-zBe5Lbs",
        "outputId": "248beabf-e739-4616-b928-cd9ff7aad797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete X : \n",
            " [[0.17 8.92 2.85 2.99]\n",
            " [7.92 3.24 8.65 4.48]\n",
            " [5.48 3.57 1.12 1.42]\n",
            " [4.45 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "X_with_null : \n",
            " [[0.17  nan 2.85 2.99]\n",
            " [7.92  nan 8.65  nan]\n",
            " [ nan  nan 1.12 1.42]\n",
            " [ nan 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "Complete data after imputation by non_negative_latent_factor: \n",
            " [[ 0.17   0.235  2.85   2.99 ]\n",
            " [ 7.92   9.428  8.65  10.738]\n",
            " [ 0.906  1.078  1.12   1.42 ]\n",
            " [ 5.593  7.32   4.6    5.93 ]\n",
            " [ 3.37   4.54   1.87   4.09 ]]\n",
            "\n",
            "Complete data after imputation by gain: \n",
            "       0     1     2     3\n",
            "0  0.17  4.57  2.85  2.99\n",
            "1  7.92  5.84  8.65  4.05\n",
            "2  1.29  4.57  1.12  1.42\n",
            "3  3.75  7.32  4.60  5.93\n",
            "4  3.37  4.54  1.87  4.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3 : Select Featurs"
      ],
      "metadata": {
        "id": "lb7eMgONam2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full phase autoencoder\n"
      ],
      "metadata": {
        "id": "slGFvUqH59kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Encoder Layer Class\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "# Autoencoder Class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes=None):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = EncoderLayer(input_size, hidden_size)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes:\n",
        "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        if self.num_classes:\n",
        "            logits = self.classifier(encoded)\n",
        "            return decoded, logits\n",
        "        else:\n",
        "            return decoded\n",
        "\n",
        "# Function to generate random array data\n",
        "def generate_random_array(shape, low, high, round_val, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=shape).round(round_val)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "# Function to generate random null values in the dataset\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "Pf2mTxSx6H6i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 1: Unsupervised Learning"
      ],
      "metadata": {
        "id": "FyW9tTBr29-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 1: Unsupervised Learning\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=1)\n",
        "\n",
        "# Create an instance of the Autoencoder model for Scenario 1\n",
        "autoencoder_unsupervised = Autoencoder(input_size=10, hidden_size=5)\n",
        "\n",
        "# Train the Autoencoder model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_unsupervised.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    outputs = autoencoder_unsupervised(inputs)\n",
        "    loss = criterion(outputs, inputs)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_unsupervised.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "XBCkOrBz3DYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 2: Supervised Learning with 1 Class"
      ],
      "metadata": {
        "id": "eQdw_d183F_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 2: Supervised Learning with 1 Class\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=2)\n",
        "labels = np.random.randint(0, 2, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 2\n",
        "autoencoder_supervised_1class = Autoencoder(input_size=10, hidden_size=5, num_classes=1)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_1class.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_1class(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_1class.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "F05ieObr3JTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 3: Supervised Learning with Multiple Classes (3)"
      ],
      "metadata": {
        "id": "PbQAuBjO3Len"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 3: Supervised Learning with Multiple Classes (3)\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=3)\n",
        "labels = np.random.randint(0, 3, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 3\n",
        "autoencoder_supervised_multiclass = Autoencoder(input_size=10, hidden_size=5, num_classes=3)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_multiclass.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels).long()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_multiclass(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_multiclass.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "09S0N6PP3PNY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}