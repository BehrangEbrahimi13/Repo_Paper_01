{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cVX43SalTOlI",
        "aqh3Unrhrexe",
        "_QSlsNoFtwkm",
        "cbDeRI_665k3",
        "ETgB4s0O3RNa",
        "slGFvUqH59kC",
        "FyW9tTBr29-h",
        "eQdw_d183F_9",
        "PbQAuBjO3Len",
        "-bv_ikz3Ra3k"
      ],
      "authorship_tag": "ABX9TyNDcQy+Z8/W8yGxCefMwP+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehrangEbrahimi13/Repo_Paper_01/blob/%2303-Implementation-Feature-Screening/Paper_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "lH7r2TioTLMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate synthetic data"
      ],
      "metadata": {
        "id": "zsEtku6DGPuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed):\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Generate random class values according to a desired distribution\n",
        "  class_data = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Initialize an empty array to hold the feature data\n",
        "  class_dependent_feature_data = np.zeros((num_samples, num_class_dependent_features))\n",
        "\n",
        "  # Generate random values for each feature independently\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Modify the feature values based on their relationship with the class\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] += class_data * (i + 1) # You can multiply the class_data by a scaling factor to control the relationship strength\n",
        "\n",
        "  # Create a linear combination of the last two features and add to the rest of the features\n",
        "  dependent_column = np.zeros((num_class_dependent_features, 1))\n",
        "  dependent_column[num_class_dependent_features-2:, 0] = 1\n",
        "  new_dependent_feature = np.dot(class_dependent_feature_data, dependent_column)\n",
        "  feature_data = np.column_stack((class_dependent_feature_data, new_dependent_feature))\n",
        "\n",
        "  # Generate random independent features\n",
        "  class_independent_features = np.random.rand(num_samples, num_class_independent_features)\n",
        "\n",
        "  # Optional: Add some noise to the features to make them more diverse\n",
        "  class_independent_features = class_independent_features + np.random.normal(0, noise, class_independent_features.shape)\n",
        "\n",
        "  # Merge the independent features from class and feature data into a single feature_data\n",
        "  feature_data = np.column_stack((feature_data, class_independent_features))\n",
        "\n",
        "  # Merge the class and feature data into a single dataset\n",
        "  # dataset = np.column_stack((class_data, feature_data))\n",
        "\n",
        "  return class_data, feature_data\n"
      ],
      "metadata": {
        "id": "oVMBqeUPGSSr"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Null Generation for a Dataset"
      ],
      "metadata": {
        "id": "cVX43SalTOlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_random_array(shape, low, high, round, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=(shape)).round(round)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp"
      ],
      "metadata": {
        "id": "YziyJ6QCTRaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1 : PMIC\n",
        "Generally, feature selection does not work on missing data, so imputation is needed beforehand. However, considering irrelevant features severely affect imputation, we use MIC to select features on missing data by ‘‘partial sample strategy’’ (PSS), which is called **PMIC**. ‘‘Partial sample strategy’’ means using the available values of all feature variables and class variable to calculate MIC"
      ],
      "metadata": {
        "id": "fLdXDn2-Iz-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2HuUmBs5TWXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57612da7-ba8b-46b3-94a7-270141086883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minepy\n",
            "  Downloading minepy-1.2.6.tar.gz (496 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/497.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/497.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.0/497.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from minepy) (1.23.5)\n",
            "Building wheels for collected packages: minepy\n",
            "  Building wheel for minepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minepy: filename=minepy-1.2.6-cp310-cp310-linux_x86_64.whl size=187016 sha256=0f67c43bb0870bd7b0801801e1ee37d5384cc0d26f83dd37078ef3720bfdb4b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/38/a6/825bb9b9ed81e6af43a0ef80c7cfe4cafcfdbc2f5cde2959d9\n",
            "Successfully built minepy\n",
            "Installing collected packages: minepy\n",
            "Successfully installed minepy-1.2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install minepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from minepy import MINE\n",
        "\n",
        "mine = MINE()\n",
        "def pmic_feature_selection(F, C, m):\n",
        "    num_features = F.shape[1]\n",
        "    pmic_scores = np.zeros(num_features)\n",
        "\n",
        "    for i in range(num_features):\n",
        "        musk = ~np.isnan(F[:, i])\n",
        "        # Select column i without null values\n",
        "        feature_without_null = F[musk, i]\n",
        "\n",
        "        # Filter y based on non-null values in column i\n",
        "        class_without_null = C[musk]\n",
        "\n",
        "        # Calculate the MIC (Maximal Information Coefficient) score for the current feature and class\n",
        "        mine.compute_score(feature_without_null, class_without_null)\n",
        "        pmic_scores[i] = mine.mic()\n",
        "\n",
        "    # Sort the indices of pmic_scores in descending order and select the top m indices\n",
        "    top_m_features_idx = np.argsort(pmic_scores)[::-1][:m]\n",
        "    return top_m_features_idx"
      ],
      "metadata": {
        "id": "TGdIK_9NJzcS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example with synthetic data"
      ],
      "metadata": {
        "id": "drVZs9kNqD6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "num_class_dependent_features = 6\n",
        "num_class_independent_features = 3\n",
        "low = 0\n",
        "high = 1\n",
        "round = 2\n",
        "noise = 0.1\n",
        "seed = 42\n",
        "\n",
        "class_data, feature_data = generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed)\n",
        "\n",
        "selected_idx = pmic_feature_selection(feature_data, class_data, num_class_dependent_features)\n",
        "selected_features = feature_data[:, selected_idx]\n",
        "\n",
        "# print(\"\\nfeature_data: \\n\", feature_data)\n",
        "print(f\"\\nSelected {num_class_dependent_features} indices of features :\\n\", selected_idx)\n",
        "print(f\"\\nSelected {num_class_dependent_features} features :\\n\", selected_features)"
      ],
      "metadata": {
        "id": "omWdjpqAPVNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 1 with Functions"
      ],
      "metadata": {
        "id": "KaGBfJPWrGVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 48\n",
        "percentage = 0.5\n",
        "select_top = 3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=0, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "# Generate a random 5x1 array with one-digit random values (0-9)\n",
        "Y = generate_random_array((row,), 0, 10, 0, seed=48, as_dataframe=False)\n",
        "print(\"\\ny: \\n\", Y)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "selected_idx = pmic_feature_selection(X_with_null, Y, select_top)\n",
        "selected_features = X_with_null[:, selected_idx]\n",
        "\n",
        "print(\"\\nX_with_null: \\n\", X_with_null)\n",
        "print(f\"\\nSelected {select_top} indices of features ({percentage * 100} percent null):\\n\", selected_idx)\n",
        "print(f\"\\nSelected {select_top} features ({percentage * 100} percent null):\\n\", selected_features)"
      ],
      "metadata": {
        "id": "A3gBqNfbih7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 : Imputation for the missing data"
      ],
      "metadata": {
        "id": "aqh3Unrhrexe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-negative Latent Factor\n",
        "\n",
        "Description:\n",
        "*   R: Incomplete data matrix of shape (n, m).\n",
        "*   d: Rank of the non-negative latent factors.\n",
        "*   lambda1, lambda2: Regularization parameters.\n",
        "*   max_iter: Maximum number of iterations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_QSlsNoFtwkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter):\n",
        "    # Initialize non-negative matrix P randomly\n",
        "    n, m = R.shape\n",
        "    R_copy = np.copy(R)\n",
        "    P = np.random.rand(n, d)\n",
        "\n",
        "    # Initialize non-negative matrix Q randomly\n",
        "    Q = np.random.rand(d, m)\n",
        "\n",
        "    # Initialize I according to (17)\n",
        "    I = np.ones((n, m))\n",
        "    R_nan_mask = np.isnan(R_copy)\n",
        "    I[R_nan_mask] = 0\n",
        "\n",
        "    # Set zero for Null\n",
        "    R_copy[R_nan_mask] = 0\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    iter = 0\n",
        "\n",
        "    # Convergence criterion\n",
        "    converge = False\n",
        "\n",
        "    while not converge and iter < max_iter:\n",
        "        # Update P according to (22)\n",
        "        P_new = P * ((I * R_copy) @ Q.T) / ((I * (P @ Q)) @ Q.T + lambda1 * P)\n",
        "\n",
        "        # Update Q according to (23)\n",
        "        Q_new = Q * (P_new.T @ (I * R_copy)) / (P_new.T @ (I * (P_new @ Q)) + lambda2 * Q)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.allclose(P, P_new) and np.allclose(Q, Q_new):\n",
        "            converge = True\n",
        "\n",
        "        # Update P and Q\n",
        "        P = P_new\n",
        "        Q = Q_new\n",
        "\n",
        "        # Increment iteration counter\n",
        "        iter += 1\n",
        "\n",
        "    # Impute R by (11) and obtain R_cpl\n",
        "    PQ = np.round( P @ Q, decimals=3)\n",
        "    R_cpl = np.where(R_nan_mask, PQ, R)\n",
        "\n",
        "    return R_cpl\n"
      ],
      "metadata": {
        "id": "gMXdVUfztlRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperimpute"
      ],
      "metadata": {
        "id": "cbDeRI_665k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperimpute"
      ],
      "metadata": {
        "id": "JGuFnGw4664W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hyperimpute.plugins.imputers import Imputers\n",
        "imputers = Imputers()\n",
        "\n",
        "X = pd.DataFrame([[1, 4, 7, 10], [4, 7, np.nan, np.nan], [3, 6, 9, 12], [8, 11, 14, 17]])\n",
        "\n",
        "method = \"gain\"\n",
        "\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(X.copy()).round(2)\n",
        "\n",
        "print(method, out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Dcu9uw7OUP",
        "outputId": "61580f22-99c2-400d-e8b7-66c8c1130de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gain      0     1     2      3\n",
            "0  1.0   4.0   7.0  10.00\n",
            "1  4.0   7.0  10.3  13.45\n",
            "2  3.0   6.0   9.0  12.00\n",
            "3  8.0  11.0  14.0  17.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 2 Non-negative Latent Factor and GAIN with Functions"
      ],
      "metadata": {
        "id": "ETgB4s0O3RNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 48\n",
        "round = 2\n",
        "percentage = 0.3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=round, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "print(\"\\nX_with_null : \\n\", X_with_null)\n",
        "\n",
        "R = np.copy(X_with_null)\n",
        "d = 2\n",
        "lambda1 = 0.1\n",
        "lambda2 = 0.2\n",
        "max_iter = 100\n",
        "\n",
        "# Code execution\n",
        "R_cpl = Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter)\n",
        "print(\"\\nComplete data after imputation by non_negative_latent_factor: \\n\", R_cpl)\n",
        "\n",
        "\n",
        "method = \"gain\"\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(R.copy()).round(round)\n",
        "\n",
        "print(f'\\nComplete data after imputation by {method}: \\n', out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9A-zBe5Lbs",
        "outputId": "248beabf-e739-4616-b928-cd9ff7aad797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete X : \n",
            " [[0.17 8.92 2.85 2.99]\n",
            " [7.92 3.24 8.65 4.48]\n",
            " [5.48 3.57 1.12 1.42]\n",
            " [4.45 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "X_with_null : \n",
            " [[0.17  nan 2.85 2.99]\n",
            " [7.92  nan 8.65  nan]\n",
            " [ nan  nan 1.12 1.42]\n",
            " [ nan 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "Complete data after imputation by non_negative_latent_factor: \n",
            " [[ 0.17   0.235  2.85   2.99 ]\n",
            " [ 7.92   9.428  8.65  10.738]\n",
            " [ 0.906  1.078  1.12   1.42 ]\n",
            " [ 5.593  7.32   4.6    5.93 ]\n",
            " [ 3.37   4.54   1.87   4.09 ]]\n",
            "\n",
            "Complete data after imputation by gain: \n",
            "       0     1     2     3\n",
            "0  0.17  4.57  2.85  2.99\n",
            "1  7.92  5.84  8.65  4.05\n",
            "2  1.29  4.57  1.12  1.42\n",
            "3  3.75  7.32  4.60  5.93\n",
            "4  3.37  4.54  1.87  4.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3 : Select Featurs"
      ],
      "metadata": {
        "id": "lb7eMgONam2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Dimension Reduction and Feature Extraction\n"
      ],
      "metadata": {
        "id": "slGFvUqH59kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Encoder Layer Class\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "# Autoencoder Class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes=None):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = EncoderLayer(input_size, hidden_size)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes:\n",
        "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        if self.num_classes:\n",
        "            logits = self.classifier(encoded)\n",
        "            return decoded, logits\n",
        "        else:\n",
        "            return decoded\n",
        "\n",
        "# Function to generate random array data\n",
        "def generate_random_array(shape, low, high, round_val, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=shape).round(round_val)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "# Function to generate random null values in the dataset\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "Pf2mTxSx6H6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 1: Unsupervised Learning"
      ],
      "metadata": {
        "id": "FyW9tTBr29-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 1: Unsupervised Learning\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=1)\n",
        "\n",
        "# Create an instance of the Autoencoder model for Scenario 1\n",
        "autoencoder_unsupervised = Autoencoder(input_size=10, hidden_size=5)\n",
        "\n",
        "# Train the Autoencoder model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_unsupervised.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    outputs = autoencoder_unsupervised(inputs)\n",
        "    loss = criterion(outputs, inputs)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_unsupervised.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "XBCkOrBz3DYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 2: Supervised Learning with 1 Class"
      ],
      "metadata": {
        "id": "eQdw_d183F_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 2: Supervised Learning with 1 Class\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=2)\n",
        "labels = np.random.randint(0, 2, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 2\n",
        "autoencoder_supervised_1class = Autoencoder(input_size=10, hidden_size=5, num_classes=1)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_1class.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_1class(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_1class.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "F05ieObr3JTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z8ibCPrGV38u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 3: Supervised Learning with Multiple Classes (3)"
      ],
      "metadata": {
        "id": "PbQAuBjO3Len"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 3: Supervised Learning with Multiple Classes (3)\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=3)\n",
        "labels = np.random.randint(0, 3, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 3\n",
        "autoencoder_supervised_multiclass = Autoencoder(input_size=10, hidden_size=5, num_classes=3)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_multiclass.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels).long()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_multiclass(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_multiclass.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "09S0N6PP3PNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Feature Screening\n",
        "\n",
        "Screen all the features via multivariate rank distance correlation learning to select the relevant ones.\n",
        "\n",
        "The reason for using feature screening instead of neural network is the small amount of training samples.\n",
        "\n",
        "When training samples are limited, neural network can easily result in overfitting; however, if we only consider one feature at a time, the dependence between the feature and xencoded can still be well estimated even when the sample size is small."
      ],
      "metadata": {
        "id": "-bv_ikz3Ra3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def multivariate_rank_distance(X, Y=None):\n",
        "    \"\"\"\n",
        "    Compute the multivariate rank distance correlation (MrDc) between two sets of variables X and Y.\n",
        "    If Y is not provided, it computes the unsupervised MrDc.\n",
        "    Args:\n",
        "        X (ndarray): Predictor variables, shape (n, p).\n",
        "        Y (ndarray): Response variables, shape (n, m).\n",
        "    Returns:\n",
        "        float: The MrDc value.\n",
        "    \"\"\"\n",
        "    n, p = X.shape\n",
        "\n",
        "    if Y is None:  # Unsupervised MrDc\n",
        "        Y = X\n",
        "\n",
        "    # Compute pairwise distance matrices\n",
        "    X_dist = squareform(pdist(X, 'euclidean'))\n",
        "    Y_dist = squareform(pdist(Y, 'euclidean'))\n",
        "\n",
        "    # Compute ranks\n",
        "    X_ranks = np.argsort(X_dist, axis=0).argsort(axis=0)\n",
        "    Y_ranks = np.argsort(Y_dist, axis=0).argsort(axis=0)\n",
        "\n",
        "    # Compute rank distance correlation\n",
        "    R = np.sum((X_ranks - Y_ranks) ** 2) / (n * (n - 1))\n",
        "    return np.sqrt(R)\n",
        "\n",
        "def mrdc_sis(X, k, y=None):\n",
        "    \"\"\"\n",
        "    Perform feature screening using the MrDc-SIS method.\n",
        "    Args:\n",
        "        X (ndarray): Predictor variables, shape (n, p).\n",
        "        k (int): Number of top-ranked variables to select.\n",
        "        y (ndarray): Response variable (optional), shape (n,).\n",
        "    Returns:\n",
        "        ndarray: Indices of the selected variables.\n",
        "    \"\"\"\n",
        "    p = X.shape[1]\n",
        "    scores = np.zeros(p)\n",
        "\n",
        "    # Compute MrDc scores for each predictor variable\n",
        "    for i in range(p):\n",
        "        if y is not None:  # Supervised MrDc\n",
        "            scores[i] = multivariate_rank_distance(X[:, i:i+1], y.reshape(-1, 1))\n",
        "        else:  # Unsupervised MrDc\n",
        "            scores[i] = multivariate_rank_distance(X[:, i:i+1])\n",
        "\n",
        "    # Rank the variables based on their scores\n",
        "    ranks = np.argsort(scores)\n",
        "    selected_indices = ranks[:k]\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "# Example usage for both supervised and unsupervised feature screening\n",
        "# Generate random data\n",
        "np.random.seed(0)\n",
        "n = 100  # Sample size\n",
        "p = 10   # Number of predictor variables\n",
        "m = 3    # Number of response variables\n",
        "X = np.random.rand(n, p)\n",
        "y = np.random.rand(n)\n",
        "\n",
        "# Perform supervised feature screening with MrDc-SIS\n",
        "k = 3  # Number of top-ranked variables to select\n",
        "selected_indices_supervised = mrdc_sis(X, k, y)\n",
        "selected_variables_supervised = X[:, selected_indices_supervised]\n",
        "\n",
        "print(\"Selected variables indices (supervised):\", selected_indices_supervised)\n",
        "print(\"Selected variables (supervised):\")\n",
        "print(selected_variables_supervised)\n",
        "\n",
        "# Perform unsupervised feature screening with MrDc-SIS\n",
        "selected_indices_unsupervised = mrdc_sis(X, k)\n",
        "selected_variables_unsupervised = X[:, selected_indices_unsupervised]\n",
        "\n",
        "print(\"Selected variables indices (unsupervised):\", selected_indices_unsupervised)\n",
        "print(\"Selected variables (unsupervised):\")\n",
        "print(selected_variables_unsupervised)"
      ],
      "metadata": {
        "id": "vbcctKR3kYXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# Generate a random dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(5, 3)\n",
        "\n",
        "# Compute pairwise distances using Euclidean distance metric\n",
        "distances = pdist(X, metric='euclidean')\n",
        "\n",
        "print(\"X : \", X)\n",
        "\n",
        "print(\"Pairwise distances:\")\n",
        "print(distances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dql3I5E7nxpi",
        "outputId": "09f26f57-67bb-46ae-94bf-280fc9c44b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X :  [[0.5488135  0.71518937 0.60276338]\n",
            " [0.54488318 0.4236548  0.64589411]\n",
            " [0.43758721 0.891773   0.96366276]\n",
            " [0.38344152 0.79172504 0.52889492]\n",
            " [0.56804456 0.92559664 0.07103606]]\n",
            "Pairwise distances:\n",
            "[0.29473397 0.41689499 0.19662693 0.57216693 0.57586803 0.41860234\n",
            " 0.76350759 0.44940452 0.90274337 0.51150232]\n"
          ]
        }
      ]
    }
  ]
}