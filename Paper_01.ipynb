{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zsEtku6DGPuP",
        "cVX43SalTOlI",
        "fLdXDn2-Iz-E",
        "drVZs9kNqD6T",
        "KaGBfJPWrGVg",
        "_QSlsNoFtwkm",
        "cbDeRI_665k3",
        "ETgB4s0O3RNa",
        "6rsWIbH7vPeO",
        "slGFvUqH59kC",
        "FyW9tTBr29-h",
        "eQdw_d183F_9",
        "PbQAuBjO3Len"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehrangEbrahimi13/Repo_Paper_01/blob/%2303-Implementation-Feature-Screening/Paper_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "lH7r2TioTLMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate synthetic data"
      ],
      "metadata": {
        "id": "zsEtku6DGPuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed):\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Generate random class values according to a desired distribution\n",
        "  class_data = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Initialize an empty array to hold the feature data\n",
        "  class_dependent_feature_data = np.zeros((num_samples, num_class_dependent_features))\n",
        "\n",
        "  # Generate random values for each feature independently\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Modify the feature values based on their relationship with the class\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] += class_data * (i + 1) # You can multiply the class_data by a scaling factor to control the relationship strength\n",
        "\n",
        "  # Create a linear combination of the last two features and add to the rest of the features\n",
        "  dependent_column = np.zeros((num_class_dependent_features, 1))\n",
        "  dependent_column[num_class_dependent_features-2:, 0] = 1\n",
        "  new_dependent_feature = np.dot(class_dependent_feature_data, dependent_column).round(round)\n",
        "  feature_data = np.column_stack((class_dependent_feature_data, new_dependent_feature))\n",
        "\n",
        "  # Generate random independent features\n",
        "  class_independent_features = np.random.rand(num_samples, num_class_independent_features)\n",
        "\n",
        "  # Optional: Add some noise to the features to make them more diverse\n",
        "  class_independent_features = (class_independent_features + np.random.normal(0, noise, class_independent_features.shape)).round(round)\n",
        "\n",
        "  # Merge the independent features from class and feature data into a single feature_data\n",
        "  feature_data = np.column_stack((feature_data, class_independent_features))\n",
        "\n",
        "  # Merge the class and feature data into a single dataset\n",
        "  # dataset = np.column_stack((class_data, feature_data))\n",
        "\n",
        "  return class_data, feature_data\n"
      ],
      "metadata": {
        "id": "oVMBqeUPGSSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Null Generation for a Dataset"
      ],
      "metadata": {
        "id": "cVX43SalTOlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_random_array(shape, low, high, round, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=(shape)).round(round)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    # missing_mask = np.random.rand(n_samples, n_features) < 0.2\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp"
      ],
      "metadata": {
        "id": "YziyJ6QCTRaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1 : PMIC\n",
        "Generally, feature selection does not work on missing data, so imputation is needed beforehand. However, considering irrelevant features severely affect imputation, we use MIC to select features on missing data by ‘‘partial sample strategy’’ (PSS), which is called **PMIC**. ‘‘Partial sample strategy’’ means using the available values of all feature variables and class variable to calculate MIC"
      ],
      "metadata": {
        "id": "fLdXDn2-Iz-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HuUmBs5TWXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57612da7-ba8b-46b3-94a7-270141086883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minepy\n",
            "  Downloading minepy-1.2.6.tar.gz (496 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/497.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/497.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.0/497.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from minepy) (1.23.5)\n",
            "Building wheels for collected packages: minepy\n",
            "  Building wheel for minepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minepy: filename=minepy-1.2.6-cp310-cp310-linux_x86_64.whl size=187016 sha256=0f67c43bb0870bd7b0801801e1ee37d5384cc0d26f83dd37078ef3720bfdb4b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/38/a6/825bb9b9ed81e6af43a0ef80c7cfe4cafcfdbc2f5cde2959d9\n",
            "Successfully built minepy\n",
            "Installing collected packages: minepy\n",
            "Successfully installed minepy-1.2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install minepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from minepy import MINE\n",
        "\n",
        "mine = MINE()\n",
        "def pmic_feature_selection(F, C, m):\n",
        "    num_features = F.shape[1]\n",
        "    pmic_scores = np.zeros(num_features)\n",
        "\n",
        "    for i in range(num_features):\n",
        "        musk = ~np.isnan(F[:, i])\n",
        "        # Select column i without null values\n",
        "        feature_without_null = F[musk, i]\n",
        "\n",
        "        # Filter y based on non-null values in column i\n",
        "        class_without_null = C[musk]\n",
        "\n",
        "        # Calculate the MIC (Maximal Information Coefficient) score for the current feature and class\n",
        "        mine.compute_score(feature_without_null, class_without_null)\n",
        "        pmic_scores[i] = mine.mic()\n",
        "\n",
        "    # Sort the indices of pmic_scores in descending order and select the top m indices\n",
        "    top_m_features_idx = np.argsort(pmic_scores)[::-1][:m]\n",
        "    return top_m_features_idx"
      ],
      "metadata": {
        "id": "TGdIK_9NJzcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Module 1 with synthetic data"
      ],
      "metadata": {
        "id": "drVZs9kNqD6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "num_class_dependent_features = 6\n",
        "num_class_independent_features = 3\n",
        "low = 0\n",
        "high = 1\n",
        "round = 2\n",
        "noise = 0.1\n",
        "seed = 42\n",
        "\n",
        "class_data, feature_data = generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed)\n",
        "\n",
        "selected_idx = pmic_feature_selection(feature_data, class_data, num_class_dependent_features)\n",
        "selected_features = feature_data[:, selected_idx]\n",
        "\n",
        "# print(\"\\nfeature_data: \\n\", feature_data)\n",
        "print(f\"\\nSelected {num_class_dependent_features} indices of features :\\n\", selected_idx)\n",
        "print(f\"\\nSelected {num_class_dependent_features} features :\\n\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omWdjpqAPVNl",
        "outputId": "d1390868-d49f-413d-8036-dbed6039541c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected 6 indices of features :\n",
            " [6 5 4 3 2 1]\n",
            "\n",
            "Selected 6 features :\n",
            " [[ 5.11  2.87  2.24  2.05  1.78  1.  ]\n",
            " [11.09  5.87  5.22  4.61  3.65  2.15]\n",
            " [ 9.75  5.25  4.5   3.68  2.44  2.37]\n",
            " ...\n",
            " [ 3.22  1.75  1.47  0.89  0.81  0.59]\n",
            " [11.29  6.05  5.24  4.62  3.38  2.19]\n",
            " [ 5.72  3.32  2.4   2.4   1.51  1.77]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 1 with Functions"
      ],
      "metadata": {
        "id": "KaGBfJPWrGVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 48\n",
        "percentage = 0.5\n",
        "select_top = 3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=0, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "# Generate a random 5x1 array with one-digit random values (0-9)\n",
        "Y = generate_random_array((row,), 0, 10, 0, seed=48, as_dataframe=False)\n",
        "print(\"\\ny: \\n\", Y)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "selected_idx = pmic_feature_selection(X_with_null, Y, select_top)\n",
        "selected_features = X_with_null[:, selected_idx]\n",
        "\n",
        "print(\"\\nX_with_null: \\n\", X_with_null)\n",
        "print(f\"\\nSelected {select_top} indices of features ({percentage * 100} percent null):\\n\", selected_idx)\n",
        "print(f\"\\nSelected {select_top} features ({percentage * 100} percent null):\\n\", selected_features)"
      ],
      "metadata": {
        "id": "A3gBqNfbih7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 : Imputation for the missing data"
      ],
      "metadata": {
        "id": "aqh3Unrhrexe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-negative Latent Factor\n",
        "\n",
        "Description:\n",
        "*   R: Incomplete data matrix of shape (n, m).\n",
        "*   d: Rank of the non-negative latent factors.\n",
        "*   lambda1, lambda2: Regularization parameters.\n",
        "*   max_iter: Maximum number of iterations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_QSlsNoFtwkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter):\n",
        "    # Initialize non-negative matrix P randomly\n",
        "    n, m = R.shape\n",
        "    R_copy = np.copy(R)\n",
        "    P = np.random.rand(n, d)\n",
        "\n",
        "    # Initialize non-negative matrix Q randomly\n",
        "    Q = np.random.rand(d, m)\n",
        "\n",
        "    # Initialize I according to (17)\n",
        "    I = np.ones((n, m))\n",
        "    R_nan_mask = np.isnan(R_copy)\n",
        "    I[R_nan_mask] = 0\n",
        "\n",
        "    # Set zero for Null\n",
        "    R_copy[R_nan_mask] = 0\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    iter = 0\n",
        "\n",
        "    # Convergence criterion\n",
        "    converge = False\n",
        "\n",
        "    while not converge and iter < max_iter:\n",
        "        # Update P according to (22)\n",
        "        P_new = P * ((I * R_copy) @ Q.T) / ((I * (P @ Q)) @ Q.T + lambda1 * P)\n",
        "\n",
        "        # Update Q according to (23)\n",
        "        Q_new = Q * (P_new.T @ (I * R_copy)) / (P_new.T @ (I * (P_new @ Q)) + lambda2 * Q)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.allclose(P, P_new) and np.allclose(Q, Q_new):\n",
        "            converge = True\n",
        "\n",
        "        # Update P and Q\n",
        "        P = P_new\n",
        "        Q = Q_new\n",
        "\n",
        "        # Increment iteration counter\n",
        "        iter += 1\n",
        "\n",
        "    # Impute R by (11) and obtain R_cpl\n",
        "    PQ = np.round( P @ Q, decimals=3)\n",
        "    R_cpl = np.where(R_nan_mask, PQ, R)\n",
        "\n",
        "    return R_cpl\n"
      ],
      "metadata": {
        "id": "gMXdVUfztlRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperimpute"
      ],
      "metadata": {
        "id": "cbDeRI_665k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperimpute"
      ],
      "metadata": {
        "id": "JGuFnGw4664W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hyperimpute.plugins.imputers import Imputers\n",
        "imputers = Imputers()\n",
        "\n",
        "X = pd.DataFrame([[1, 4, 7, 10], [4, 7, np.nan, np.nan], [3, 6, 9, 12], [8, 11, 14, 17]])\n",
        "\n",
        "method = \"gain\"\n",
        "\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(X.copy()).round(2)\n",
        "\n",
        "print(method, out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Dcu9uw7OUP",
        "outputId": "0f56cb67-a32f-420a-f4f2-ff174df3cac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gain      0     1     2      3\n",
            "0  1.0   4.0   7.0  10.00\n",
            "1  4.0   7.0  10.3  13.45\n",
            "2  3.0   6.0   9.0  12.00\n",
            "3  8.0  11.0  14.0  17.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 2 Non-negative Latent Factor and GAIN with Functions"
      ],
      "metadata": {
        "id": "ETgB4s0O3RNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperimpute.plugins.imputers import Imputers\n",
        "\n",
        "seed = 48\n",
        "round = 2\n",
        "percentage = 0.3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=round, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "print(\"\\nX_with_null : \\n\", X_with_null)\n",
        "\n",
        "R = np.copy(X_with_null)\n",
        "d = 2\n",
        "lambda1 = 0.1\n",
        "lambda2 = 0.2\n",
        "max_iter = 100\n",
        "\n",
        "# Code execution\n",
        "R_cpl = Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter)\n",
        "print(\"\\nComplete data after imputation by non_negative_latent_factor: \\n\", R_cpl)\n",
        "\n",
        "\n",
        "method = \"gain\"\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(R.copy()).round(round)\n",
        "\n",
        "print(f'\\nComplete data after imputation by {method}: \\n', out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9A-zBe5Lbs",
        "outputId": "75d277ee-0f59-4509-f72b-1fb23dc7176f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete X : \n",
            " [[0.17 8.92 2.85 2.99]\n",
            " [7.92 3.24 8.65 4.48]\n",
            " [5.48 3.57 1.12 1.42]\n",
            " [4.45 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "X_with_null : \n",
            " [[0.17  nan 2.85 2.99]\n",
            " [7.92  nan 8.65  nan]\n",
            " [ nan  nan 1.12 1.42]\n",
            " [ nan 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "Complete data after imputation by non_negative_latent_factor: \n",
            " [[ 0.17   0.235  2.85   2.99 ]\n",
            " [ 7.92   9.428  8.65  10.738]\n",
            " [ 0.906  1.078  1.12   1.42 ]\n",
            " [ 5.593  7.32   4.6    5.93 ]\n",
            " [ 3.37   4.54   1.87   4.09 ]]\n",
            "\n",
            "Complete data after imputation by gain: \n",
            "       0     1     2     3\n",
            "0  0.17  4.57  2.85  2.99\n",
            "1  7.92  5.84  8.65  4.05\n",
            "2  1.29  4.57  1.12  1.42\n",
            "3  3.75  7.32  4.60  5.93\n",
            "4  3.37  4.54  1.87  4.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 2 with synthetic data"
      ],
      "metadata": {
        "id": "6rsWIbH7vPeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fancyimpute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78d_s3SBHDu",
        "outputId": "182b9727-51f1-4a9e-bc1a-4fadd966aad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fancyimpute in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: knnimpute>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (0.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.2.2)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (7.4.3)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.3.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (3.2.0)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (3.2.4)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (67.7.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (2.0.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.7.post0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperimpute.plugins.imputers import Imputers\n",
        "imputers = Imputers()\n",
        "\n",
        "imputers.list()\n",
        "\n",
        "num_samples = 500\n",
        "num_class_dependent_features = 100\n",
        "num_class_independent_features = 50\n",
        "low = 0\n",
        "high = 1\n",
        "round = 2\n",
        "noise = 0.1\n",
        "seed = 42\n",
        "percentage = 0.3\n",
        "\n",
        "class_data, feature_data = generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed)\n",
        "print (\"max : \", max(map(max, feature_data)))\n",
        "\n",
        "# Normalize feature_data data between 0 and 1 for each column\n",
        "# normalized_data = np.zeros_like(feature_data)\n",
        "\n",
        "# for i in range(feature_data.shape[1]):\n",
        "#     column = feature_data[:, i]\n",
        "#     min_val = np.min(column)\n",
        "#     max_val = np.max(column)\n",
        "#     if max_val - min_val == 0:\n",
        "#         normalized_column = np.zeros_like(column)\n",
        "#     else:\n",
        "#         normalized_column = (column - min_val) / (max_val - min_val)\n",
        "#     normalized_data[:, i] = normalized_column\n",
        "\n",
        "\n",
        "\n",
        "# print(\"\\nFeature_data : \\n\", feature_data)\n",
        "\n",
        "feature_data_with_null = generate_random_nulls(dataset=feature_data, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "# print(\"\\nfeature_data_with_null : \\n\", feature_data_with_null)\n",
        "\n",
        "# selected_idx = pmic_feature_selection(feature_data, class_data, num_class_dependent_features)\n",
        "# selected_features = feature_data[:, selected_idx]\n",
        "R = np.copy(feature_data_with_null)\n",
        "d = 2\n",
        "lambda1 = 0.1\n",
        "lambda2 = 0.2\n",
        "max_iter = 100\n",
        "\n",
        "# Code execution\n",
        "R_cpl = Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter)\n",
        "# print(\"\\nComplete data after imputation by non_negative_latent_factor: \\n\", R_cpl)\n",
        "\n",
        "# Implement \"gain\" using hyperimpute\n",
        "method = \"gain\"\n",
        "plugin = Imputers().get(method)\n",
        "gain_out = plugin.fit_transform(R.copy()).round(round)\n",
        "\n",
        "# method = \"miwae\"\n",
        "# plugin = Imputers().get(\"miwae\")\n",
        "# miwae_out = plugin.fit_transform(R.copy())\n",
        "\n",
        "\n",
        "# print(f'\\nComplete data after imputation by {method}: \\n', gain_out)\n",
        "\n",
        "# Implement MICE using fancyimpute\n",
        "mice_imputer = IterativeImputer()\n",
        "X_imputed_mice = mice_imputer.fit_transform(feature_data_with_null)\n",
        "\n",
        "\n",
        "# Error calculation:\n",
        "\n",
        "# For all these error metrics, smaller values are generally considered better,\n",
        "# except for the R-squared metric where higher values indicate a better fit.\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Mean Absolute Error (MAE): It is the average absolute difference between corresponding elements of the two matrices.\n",
        "mae_non_negative_latent_factor = mean_absolute_error(feature_data, R_cpl)\n",
        "print(\"\\n mae error for non_negative_latent_factor method : \",mae_non_negative_latent_factor)\n",
        "mae_mice = mean_absolute_error(feature_data, X_imputed_mice)\n",
        "print(\"\\n mae error for mice method : \",mae_mice)\n",
        "mae_gain = mean_absolute_error(feature_data, gain_out)\n",
        "print(\"\\n mae error for gain method : \",mae_gain)\n",
        "\n",
        "# Mean Squared Error (MSE): It is the average of the squared differences between corresponding elements of the two matrices.\n",
        "mse_non_negative_latent_factor = mean_squared_error(feature_data.flatten(), R_cpl.flatten())\n",
        "print(\"\\n mse error for non_negative_latent_factor method : \",mse_non_negative_latent_factor)\n",
        "mse_mice = mean_squared_error(feature_data.flatten(), X_imputed_mice.flatten())\n",
        "print(\"\\n mse error for mice method : \",mse_mice)\n",
        "mse_gain = mean_squared_error(feature_data.flatten(), gain_out.to_numpy().flatten())\n",
        "print(\"\\n mse error for gain method : \",mse_gain)\n",
        "\n",
        "# Root Mean Squared Error (RMSE): It is the square root of the MSE.\n",
        "rmse_non_negative_latent_factor = math.sqrt(mse_non_negative_latent_factor)\n",
        "print(\"\\n rmse error for non_negative_latent_factor method : \",rmse_non_negative_latent_factor)\n",
        "rmse_mice = math.sqrt(mse_mice)\n",
        "print(\"\\n rmse error for mice method : \",rmse_mice)\n",
        "rmse_gain = math.sqrt(mse_gain)\n",
        "print(\"\\n rmse error for gain method : \",rmse_gain)\n",
        "\n",
        "# R-squared (Coefficient of Determination): It is a statistical measure that indicates the proportion of the variance in the dependent variable\n",
        "# that is predictable from the independent variable(s).\n",
        "r2_non_negative_latent_factor = r2_score(feature_data.flatten(), R_cpl.flatten())\n",
        "print(\"\\n r2 error for non_negative_latent_factor method : \",r2_non_negative_latent_factor)\n",
        "r2_mice = r2_score(feature_data.flatten(), X_imputed_mice.flatten())\n",
        "print(\"\\n r2 error for mice method : \",r2_mice)\n",
        "r2_gain = r2_score(feature_data.flatten(), gain_out.to_numpy().flatten())\n",
        "print(\"\\n r2 error for gain method : \",r2_gain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USo5agSyq_uR",
        "outputId": "e60b11fb-7c23-458e-a0f0-8fb40ed86d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max :  198.74\n",
            "\n",
            " mae error for non_negative_latent_factor method :  0.08830417218543046\n",
            "\n",
            " mae error for mice method :  0.07670262870703903\n",
            "\n",
            " mae error for gain method :  0.18749721854304674\n",
            "\n",
            " mse error for non_negative_latent_factor method :  0.03923348282119206\n",
            "\n",
            " mse error for mice method :  0.02713880200306323\n",
            "\n",
            " mse error for gain method :  0.254268498013245\n",
            "\n",
            " rmse error for non_negative_latent_factor method :  0.19807443757636184\n",
            "\n",
            " rmse error for mice method :  0.16473858686738585\n",
            "\n",
            " rmse error for gain method :  0.5042504318423982\n",
            "\n",
            " r2 error for non_negative_latent_factor method :  0.999928075667905\n",
            "\n",
            " r2 error for mice method :  0.9999502481027028\n",
            "\n",
            " r2 error for gain method :  0.9995338651942841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3 : Select Featurs"
      ],
      "metadata": {
        "id": "lb7eMgONam2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Dimension Reduction and Feature Extraction\n"
      ],
      "metadata": {
        "id": "slGFvUqH59kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Encoder Layer Class\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "# Autoencoder Class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes=None):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = EncoderLayer(input_size, hidden_size)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes:\n",
        "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        if self.num_classes:\n",
        "            logits = self.classifier(encoded)\n",
        "            return decoded, logits\n",
        "        else:\n",
        "            return decoded\n",
        "\n",
        "# Function to generate random array data\n",
        "def generate_random_array(shape, low, high, round_val, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=shape).round(round_val)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "# Function to generate random null values in the dataset\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "Pf2mTxSx6H6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 1: Unsupervised Learning"
      ],
      "metadata": {
        "id": "FyW9tTBr29-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 1: Unsupervised Learning\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=1)\n",
        "\n",
        "# Create an instance of the Autoencoder model for Scenario 1\n",
        "autoencoder_unsupervised = Autoencoder(input_size=10, hidden_size=5)\n",
        "\n",
        "# Train the Autoencoder model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_unsupervised.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    outputs = autoencoder_unsupervised(inputs)\n",
        "    loss = criterion(outputs, inputs)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_unsupervised.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "XBCkOrBz3DYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 2: Supervised Learning with 1 Class"
      ],
      "metadata": {
        "id": "eQdw_d183F_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 2: Supervised Learning with 1 Class\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=2)\n",
        "labels = np.random.randint(0, 2, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 2\n",
        "autoencoder_supervised_1class = Autoencoder(input_size=10, hidden_size=5, num_classes=1)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_1class.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_1class(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_1class.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "F05ieObr3JTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z8ibCPrGV38u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 3: Supervised Learning with Multiple Classes (3)"
      ],
      "metadata": {
        "id": "PbQAuBjO3Len"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 3: Supervised Learning with Multiple Classes (3)\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=3)\n",
        "labels = np.random.randint(0, 3, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 3\n",
        "autoencoder_supervised_multiclass = Autoencoder(input_size=10, hidden_size=5, num_classes=3)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_multiclass.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels).long()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_multiclass(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_multiclass.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "09S0N6PP3PNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Feature Screening\n",
        "\n",
        "Screen all the features via multivariate rank distance correlation learning to select the relevant ones.\n",
        "\n",
        "The reason for using feature screening instead of neural network is the small amount of training samples.\n",
        "\n",
        "When training samples are limited, neural network can easily result in overfitting; however, if we only consider one feature at a time, the dependence between the feature and xencoded can still be well estimated even when the sample size is small."
      ],
      "metadata": {
        "id": "-bv_ikz3Ra3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def multivariate_rank_distance_correlation(X, Y):\n",
        "    n = X.size\n",
        "    ranks_X = np.apply_along_axis(rankdata, 0, X)\n",
        "    ranks_Y = np.apply_along_axis(rankdata, 0, Y)\n",
        "    d = np.sum((ranks_X - ranks_Y) ** 2)\n",
        "    rdc = 1 - (6 * d) / (n * (n ** 2 - 1))\n",
        "    return rdc\n",
        "\n",
        "X = np.random.rand(100, 10)  # Input data matrix (100 samples, 10 features)\n",
        "x_encode = np.random.rand(100, 5)  # Low-dimensional representation obtained from feature extraction (100 samples, 5 dimensions)\n",
        "\n",
        "correlation_values = []\n",
        "for feature in range(X.shape[1]):\n",
        "    repeated_X = np.repeat(X[:, feature][:, np.newaxis], x_encode.shape[1], axis=1)\n",
        "    rdc = multivariate_rank_distance_correlation(repeated_X, x_encode)\n",
        "    correlation_values.append((feature, rdc))\n",
        "\n",
        "sorted_correlation_values = sorted(correlation_values, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for feature, rdc in sorted_correlation_values:\n",
        "    print(\"Feature\", feature + 1, \"has Multivariate Rank Distance Correlation:\", rdc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-64NZzrSl29m",
        "outputId": "536bde4c-1b52-402e-b655-9dfdbbac6d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 7 has Multivariate Rank Distance Correlation: 0.96413099252397\n",
            "Feature 2 has Multivariate Rank Distance Correlation: 0.9617408709634838\n",
            "Feature 4 has Multivariate Rank Distance Correlation: 0.9617403909615638\n",
            "Feature 8 has Multivariate Rank Distance Correlation: 0.9605682262729051\n",
            "Feature 3 has Multivariate Rank Distance Correlation: 0.9597258229032917\n",
            "Feature 1 has Multivariate Rank Distance Correlation: 0.9591032604130416\n",
            "Feature 5 has Multivariate Rank Distance Correlation: 0.958874203496814\n",
            "Feature 10 has Multivariate Rank Distance Correlation: 0.9573249492997972\n",
            "Feature 9 has Multivariate Rank Distance Correlation: 0.9571961167844671\n",
            "Feature 6 has Multivariate Rank Distance Correlation: 0.9550538682154729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Implementation"
      ],
      "metadata": {
        "id": "rlNM8M4IXdQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installe libraries\n",
        "\n",
        "!pip install minepy\n",
        "!pip install hyperimpute\n",
        "!pip install fancyimpute"
      ],
      "metadata": {
        "id": "cW2K8B1FrXfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from minepy import MINE\n",
        "from hyperimpute.plugins.imputers import Imputers\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Error calculation:\n",
        "# For all these error metrics, smaller values are generally considered better,\n",
        "# except for the R-squared metric where higher values indicate a better fit.\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed):\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Generate random class values according to a desired distribution\n",
        "  class_data = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Initialize an empty array to hold the feature data\n",
        "  class_dependent_feature_data = np.zeros((num_samples, num_class_dependent_features))\n",
        "\n",
        "  # Generate random values for each feature independently\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Modify the feature values based on their relationship with the class\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] += class_data * (i + 1) # You can multiply the class_data by a scaling factor to control the relationship strength\n",
        "\n",
        "  # Create a linear combination of the last two features and add to the rest of the features\n",
        "  dependent_column = np.zeros((num_class_dependent_features, 1))\n",
        "  dependent_column[num_class_dependent_features-2:, 0] = 1\n",
        "  new_dependent_feature = np.dot(class_dependent_feature_data, dependent_column).round(round)\n",
        "  feature_data = np.column_stack((class_dependent_feature_data, new_dependent_feature))\n",
        "\n",
        "  # Generate random independent features\n",
        "  class_independent_features = np.random.rand(num_samples, num_class_independent_features)\n",
        "\n",
        "  # Optional: Add some noise to the features to make them more diverse\n",
        "  class_independent_features = (class_independent_features + np.random.normal(0, noise, class_independent_features.shape)).round(round)\n",
        "\n",
        "  # Merge the independent features from class and feature data into a single feature_data\n",
        "  feature_data = np.column_stack((feature_data, class_independent_features))\n",
        "\n",
        "  return class_data, feature_data\n",
        "\n",
        "def generate_random_array(shape, low, high, round, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=(shape)).round(round)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    # missing_mask = np.random.rand(n_samples, n_features) < 0.2\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp\n",
        "\n",
        "def pmic_feature_selection(feature_data, class_data, top=None):\n",
        "    mine = MINE()\n",
        "    num_features = feature_data.shape[1]\n",
        "    pmic_scores = np.zeros(num_features)\n",
        "\n",
        "    for i in range(num_features):\n",
        "        musk = ~np.isnan(feature_data[:, i])\n",
        "        # Select column i without null values\n",
        "        feature_without_null = feature_data[musk, i]\n",
        "\n",
        "        # Filter y based on non-null values in column i\n",
        "        class_without_null = class_data[musk]\n",
        "\n",
        "        # Calculate the MIC (Maximal Information Coefficient) score for the current feature and class\n",
        "        mine.compute_score(feature_without_null, class_without_null)\n",
        "        pmic_scores[i] = mine.mic()\n",
        "\n",
        "    top_m_features_idx = np.argsort(pmic_scores)[::-1]\n",
        "    return top_m_features_idx if top is None else top_m_features_idx[:top]\n",
        "\n",
        "# R: Incomplete data matrix of shape (n, m).\n",
        "# d: Rank of the non-negative latent factors.\n",
        "# lambda1, lambda2: Regularization parameters.\n",
        "# max_iter: Maximum number of iterations.\n",
        "def Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter):\n",
        "    # Initialize non-negative matrix P randomly\n",
        "    n, m = R.shape\n",
        "    R_copy = np.copy(R)\n",
        "    P = np.random.rand(n, d)\n",
        "\n",
        "    # Initialize non-negative matrix Q randomly\n",
        "    Q = np.random.rand(d, m)\n",
        "\n",
        "    # Initialize I according to (17)\n",
        "    I = np.ones((n, m))\n",
        "    R_nan_mask = np.isnan(R_copy)\n",
        "    I[R_nan_mask] = 0\n",
        "\n",
        "    # Set zero for Null\n",
        "    R_copy[R_nan_mask] = 0\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    iter = 0\n",
        "\n",
        "    # Convergence criterion\n",
        "    converge = False\n",
        "\n",
        "    while not converge and iter < max_iter:\n",
        "        # Update P according to (22)\n",
        "        P_new = P * ((I * R_copy) @ Q.T) / ((I * (P @ Q)) @ Q.T + lambda1 * P)\n",
        "\n",
        "        # Update Q according to (23)\n",
        "        Q_new = Q * (P_new.T @ (I * R_copy)) / (P_new.T @ (I * (P_new @ Q)) + lambda2 * Q)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.allclose(P, P_new) and np.allclose(Q, Q_new):\n",
        "            converge = True\n",
        "\n",
        "        # Update P and Q\n",
        "        P = P_new\n",
        "        Q = Q_new\n",
        "\n",
        "        # Increment iteration counter\n",
        "        iter += 1\n",
        "\n",
        "    # Impute R by (11) and obtain R_cpl\n",
        "    PQ = np.round( P @ Q, decimals=3)\n",
        "    R_cpl = np.where(R_nan_mask, PQ, R)\n",
        "\n",
        "    return R_cpl\n",
        "\n",
        "# Encoder Layer Class\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "# Autoencoder Class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes=None):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = EncoderLayer(input_size, hidden_size)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes:\n",
        "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        if self.num_classes:\n",
        "            logits = self.classifier(encoded)\n",
        "            return decoded, logits\n",
        "        else:\n",
        "            return decoded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Implementing 3 Scenario Functions with Switch Case:\n",
        "# Scenario 1: Unsupervised Learning\n",
        "# Scenario 2: Supervised Learning with 1 Class\n",
        "# Scenario 3: Supervised Learning with Multiple Classes (3)\n",
        "\n",
        "\n",
        "# Implement: remove redundant features method for feature screening\n",
        "# correlation matrix\n",
        "# https://poe.com/s/UIk3K6D0l8RHdzvkTHNi\n",
        "# https://poe.com/s/BePgttFGioya3D2Yuwua\n",
        "\n",
        "\n",
        "\n",
        "def multivariate_rank_distance_correlation(X, Y):\n",
        "    n = X.size\n",
        "    ranks_X = np.apply_along_axis(rankdata, 0, X)\n",
        "    ranks_Y = np.apply_along_axis(rankdata, 0, Y)\n",
        "    d = np.sum((ranks_X - ranks_Y) ** 2)\n",
        "    rdc = 1 - (6 * d) / (n * (n ** 2 - 1))\n",
        "    return rdc\n",
        "\n",
        "def select_top_sorted_rank(orginal_data, encode_data, top=None):\n",
        "    correlation_values = []\n",
        "    for feature in range(X.shape[1]):\n",
        "        repeated_X = np.repeat(X[:, feature][:, np.newaxis], x_encode.shape[1], axis=1)\n",
        "        rdc = multivariate_rank_distance_correlation(repeated_X, x_encode)\n",
        "        correlation_values.append((feature, rdc))\n",
        "\n",
        "    sorted_correlation_values = sorted(correlation_values, key=lambda x: x[1], reverse=True)\n",
        "    for feature, rdc in sorted_correlation_values:\n",
        "        print(\"Feature\", feature + 1, \"has Multivariate Rank Distance Correlation:\", rdc)\n",
        "    # return sorted_correlation_values if top is None else sorted_correlation_values[:top]\n",
        "\n",
        "# Implement final evaluation method"
      ],
      "metadata": {
        "id": "CK067zJpXpbO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Methods and evaluation and stor result in csv"
      ],
      "metadata": {
        "id": "bMMhT4h-8ux3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ارزیابی"
      ],
      "metadata": {
        "id": "sJz2IbJCL5tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif, f_regression\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, load_diabetes, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "results_df = pd.DataFrame(columns=['Dataset', 'Model', 'Missing Rate', 'MSE', 'MAE', 'RMSE', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
        "\n",
        "# تابعی برای ایجاد m ویژگی مصنوعی\n",
        "def create_synthetic_features(n_samples, m_features, random_state=None):\n",
        "    np.random.seed(random_state)\n",
        "    X_synthetic = np.random.rand(n_samples, m_features)\n",
        "    return X_synthetic\n",
        "\n",
        "\n",
        "# تابعی برای افزودن ویژگی‌های مصنوعی به داده‌های موجود\n",
        "def add_synthetic_features(X_original, X_synthetic, shuffle_columns=True):\n",
        "    # افزودن ویژگی‌های مصنوعی\n",
        "    combined_data = np.hstack((X_original, X_synthetic))\n",
        "    if  shuffle_columns :\n",
        "      # تولید اندیس‌های شافل شده برای ستون‌ها\n",
        "      shuffled_indices = np.random.permutation(combined_data.shape[1])\n",
        "      # اعمال شافل شده بر روی ستون‌ها\n",
        "      shuffled_data = combined_data[:, shuffled_indices]\n",
        "    return shuffled_data\n",
        "\n",
        "# تابعی برای ارزیابی مدل دسته‌بندی\n",
        "def evaluate_classification_model(dataset_name, missing_rate, X, y, feature_selection=False, k=0):\n",
        "    # تقسیم داده‌ها\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # انتخاب برترین ویژگی‌ها در صورت نیاز\n",
        "    if feature_selection:\n",
        "        selector = SelectKBest(f_classif, k=k)\n",
        "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "        X_test_selected = selector.transform(X_test)\n",
        "    else:\n",
        "        X_train_selected = X_train\n",
        "        X_test_selected = X_test\n",
        "\n",
        "    # آموزش مدل\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_selected, y_train)\n",
        "\n",
        "    # ارزیابی عملکرد\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    results_df.loc[len(results_df)] = [dataset_name, 'RandomForestClassifier', missing_rate, None, None, None, accuracy, precision, recall, f1]\n",
        "\n",
        "# تابعی برای ارزیابی دیتاست‌های رگرسیون با قابلیت انتخاب ویژگی\n",
        "def evaluate_regression_model(dataset_name, missing_rate, X, y, feature_selection=False, k=0):\n",
        "    # تقسیم داده‌ها\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # انتخاب برترین ویژگی‌ها در صورت نیاز\n",
        "    if feature_selection:\n",
        "        selector = SelectKBest(f_regression, k=k)\n",
        "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "        X_test_selected = selector.transform(X_test)\n",
        "    else:\n",
        "        X_train_selected = X_train\n",
        "        X_test_selected = X_test\n",
        "\n",
        "    # آموزش مدل\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_selected, y_train)\n",
        "\n",
        "    # ارزیابی عملکرد\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "    results_df.loc[len(results_df)] = [dataset_name, 'RandomForestRegressor', missing_rate, mse, mae, rmse, None, None, None, None]\n",
        "\n",
        "def make_data_missing(X, missing_rate, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    X_missing = X.copy()\n",
        "    missing_entries = np.random.binomial(1, p=missing_rate, size=X.shape).astype(bool)\n",
        "    X_missing[missing_entries] = np.nan\n",
        "    return X_missing\n",
        "\n",
        "datasets = {\n",
        "    'iris': (load_iris(return_X_y=True), 'classification', 4),\n",
        "    'diabetes': (load_diabetes(return_X_y=True), 'regression', 10),\n",
        "    'breast_cancer': (load_breast_cancer(return_X_y=True), 'classification', 30)\n",
        "}\n",
        "\n",
        "# m ویژگی مصنوعی که مایلید اضافه کنید\n",
        "m_features = 5\n",
        "\n",
        "# نرخ‌های داده گمشده برای آزمایش\n",
        "missing_rates = [0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "for name, ((X, y), task_type, dimensionality) in datasets.items():\n",
        "    k = dimensionality\n",
        "    n_samples = X.shape[0]\n",
        "    # ایجاد ویژگی‌های مصنوعی و اضافه کردن آن‌ها به داده‌های موجود\n",
        "    X_synthetic = create_synthetic_features(n_samples, m_features, random_state=42)\n",
        "    X_extended = add_synthetic_features(X, X_synthetic)\n",
        "    for missing_rate in missing_rates:\n",
        "        print(f'\\n{name.capitalize()} dataset with {missing_rate * 100}% missing data:')\n",
        "        X_missing = make_data_missing(X_extended, missing_rate)\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_imputed = imputer.fit_transform(X_missing)\n",
        "\n",
        "        if task_type == 'classification':\n",
        "            evaluate_classification_model(f'{name.capitalize()}',f'{missing_rate * 100}%', X_imputed, y, feature_selection=True, k=k)\n",
        "        else:\n",
        "            evaluate_regression_model(f'{name.capitalize()}',f'{missing_rate * 100}%', X_imputed, y, feature_selection=True, k=k)\n",
        "\n",
        "print(results_df)\n",
        "results_df.to_csv('evaluation_results.csv', index=False)  # ذخیره در یک فایل CSV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYM79gZY6l3-",
        "outputId": "e5c1efa6-0627-4882-895d-9b24cdcd37b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iris dataset with 30.0% missing data:\n",
            "Accuracy: 0.90, Precision: 0.93, Recall: 0.89, F1 Score: 0.90\n",
            "\n",
            "Iris dataset with 40.0% missing data:\n",
            "Accuracy: 0.90, Precision: 0.93, Recall: 0.90, F1 Score: 0.90\n",
            "\n",
            "Iris dataset with 50.0% missing data:\n",
            "Accuracy: 0.80, Precision: 0.81, Recall: 0.80, F1 Score: 0.80\n",
            "\n",
            "Iris dataset with 60.0% missing data:\n",
            "Accuracy: 0.83, Precision: 0.88, Recall: 0.84, F1 Score: 0.84\n",
            "\n",
            "Diabetes dataset with 30.0% missing data:\n",
            "MSE: 3346.36, MAE: 47.92, RMSE: 57.85\n",
            "\n",
            "Diabetes dataset with 40.0% missing data:\n",
            "MSE: 3266.15, MAE: 47.80, RMSE: 57.15\n",
            "\n",
            "Diabetes dataset with 50.0% missing data:\n",
            "MSE: 3919.92, MAE: 50.37, RMSE: 62.61\n",
            "\n",
            "Diabetes dataset with 60.0% missing data:\n",
            "MSE: 4598.29, MAE: 54.00, RMSE: 67.81\n",
            "\n",
            "Breast_cancer dataset with 30.0% missing data:\n",
            "Accuracy: 0.97, Precision: 0.98, Recall: 0.97, F1 Score: 0.97\n",
            "\n",
            "Breast_cancer dataset with 40.0% missing data:\n",
            "Accuracy: 0.96, Precision: 0.97, Recall: 0.96, F1 Score: 0.96\n",
            "\n",
            "Breast_cancer dataset with 50.0% missing data:\n",
            "Accuracy: 0.96, Precision: 0.97, Recall: 0.96, F1 Score: 0.96\n",
            "\n",
            "Breast_cancer dataset with 60.0% missing data:\n",
            "Accuracy: 0.91, Precision: 0.91, Recall: 0.91, F1 Score: 0.91\n",
            "          Dataset                   Model Missing Rate          MSE  \\\n",
            "0            Iris  RandomForestClassifier        30.0%          NaN   \n",
            "1            Iris  RandomForestClassifier        40.0%          NaN   \n",
            "2            Iris  RandomForestClassifier        50.0%          NaN   \n",
            "3            Iris  RandomForestClassifier        60.0%          NaN   \n",
            "4        Diabetes   RandomForestRegressor        30.0%  3346.356685   \n",
            "5        Diabetes   RandomForestRegressor        40.0%  3266.152771   \n",
            "6        Diabetes   RandomForestRegressor        50.0%  3919.924789   \n",
            "7        Diabetes   RandomForestRegressor        60.0%  4598.287980   \n",
            "8   Breast_cancer  RandomForestClassifier        30.0%          NaN   \n",
            "9   Breast_cancer  RandomForestClassifier        40.0%          NaN   \n",
            "10  Breast_cancer  RandomForestClassifier        50.0%          NaN   \n",
            "11  Breast_cancer  RandomForestClassifier        60.0%          NaN   \n",
            "\n",
            "          MAE       RMSE  Accuracy Precision    Recall        F1  \n",
            "0         NaN        NaN       0.9  0.928571  0.892593  0.900789  \n",
            "1         NaN        NaN       0.9  0.928571  0.896296  0.903355  \n",
            "2         NaN        NaN       0.8  0.806818  0.798653  0.801205  \n",
            "3         NaN        NaN  0.833333  0.880952  0.842424  0.837868  \n",
            "4   47.924944  57.847703      None      None      None      None  \n",
            "5   47.797753  57.150265      None      None      None      None  \n",
            "6   50.374607  62.609303      None      None      None      None  \n",
            "7   54.003060  67.810677      None      None      None      None  \n",
            "8         NaN        NaN  0.973684   0.97973  0.965116  0.971583  \n",
            "9         NaN        NaN  0.964912  0.967257  0.958074  0.962302  \n",
            "10        NaN        NaN  0.964912  0.967257  0.958074  0.962302  \n",
            "11        NaN        NaN  0.912281  0.906649  0.906649  0.906649  \n"
          ]
        }
      ]
    }
  ]
}