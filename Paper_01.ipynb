{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zsEtku6DGPuP",
        "fLdXDn2-Iz-E",
        "drVZs9kNqD6T",
        "KaGBfJPWrGVg",
        "_QSlsNoFtwkm",
        "cbDeRI_665k3",
        "ETgB4s0O3RNa",
        "slGFvUqH59kC",
        "FyW9tTBr29-h",
        "eQdw_d183F_9",
        "PbQAuBjO3Len",
        "-bv_ikz3Ra3k"
      ],
      "authorship_tag": "ABX9TyNTd4zdLxx1k6gHU67qROeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehrangEbrahimi13/Repo_Paper_01/blob/%2303-Implementation-Feature-Screening/Paper_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "lH7r2TioTLMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate synthetic data"
      ],
      "metadata": {
        "id": "zsEtku6DGPuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed):\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Generate random class values according to a desired distribution\n",
        "  class_data = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Initialize an empty array to hold the feature data\n",
        "  class_dependent_feature_data = np.zeros((num_samples, num_class_dependent_features))\n",
        "\n",
        "  # Generate random values for each feature independently\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] = np.random.uniform(low, high, size=num_samples).round(round)\n",
        "\n",
        "  # Modify the feature values based on their relationship with the class\n",
        "  for i in range(num_class_dependent_features):\n",
        "      class_dependent_feature_data[:, i] += class_data * (i + 1) # You can multiply the class_data by a scaling factor to control the relationship strength\n",
        "\n",
        "  # Create a linear combination of the last two features and add to the rest of the features\n",
        "  dependent_column = np.zeros((num_class_dependent_features, 1))\n",
        "  dependent_column[num_class_dependent_features-2:, 0] = 1\n",
        "  new_dependent_feature = np.dot(class_dependent_feature_data, dependent_column).round(round)\n",
        "  feature_data = np.column_stack((class_dependent_feature_data, new_dependent_feature))\n",
        "\n",
        "  # Generate random independent features\n",
        "  class_independent_features = np.random.rand(num_samples, num_class_independent_features)\n",
        "\n",
        "  # Optional: Add some noise to the features to make them more diverse\n",
        "  class_independent_features = (class_independent_features + np.random.normal(0, noise, class_independent_features.shape)).round(round)\n",
        "\n",
        "  # Merge the independent features from class and feature data into a single feature_data\n",
        "  feature_data = np.column_stack((feature_data, class_independent_features))\n",
        "\n",
        "  # Merge the class and feature data into a single dataset\n",
        "  # dataset = np.column_stack((class_data, feature_data))\n",
        "\n",
        "  return class_data, feature_data\n"
      ],
      "metadata": {
        "id": "oVMBqeUPGSSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Null Generation for a Dataset"
      ],
      "metadata": {
        "id": "cVX43SalTOlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_random_array(shape, low, high, round, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=(shape)).round(round)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    # missing_mask = np.random.rand(n_samples, n_features) < 0.2\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp"
      ],
      "metadata": {
        "id": "YziyJ6QCTRaC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1 : PMIC\n",
        "Generally, feature selection does not work on missing data, so imputation is needed beforehand. However, considering irrelevant features severely affect imputation, we use MIC to select features on missing data by ‘‘partial sample strategy’’ (PSS), which is called **PMIC**. ‘‘Partial sample strategy’’ means using the available values of all feature variables and class variable to calculate MIC"
      ],
      "metadata": {
        "id": "fLdXDn2-Iz-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2HuUmBs5TWXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57612da7-ba8b-46b3-94a7-270141086883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minepy\n",
            "  Downloading minepy-1.2.6.tar.gz (496 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/497.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/497.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.0/497.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from minepy) (1.23.5)\n",
            "Building wheels for collected packages: minepy\n",
            "  Building wheel for minepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minepy: filename=minepy-1.2.6-cp310-cp310-linux_x86_64.whl size=187016 sha256=0f67c43bb0870bd7b0801801e1ee37d5384cc0d26f83dd37078ef3720bfdb4b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/38/a6/825bb9b9ed81e6af43a0ef80c7cfe4cafcfdbc2f5cde2959d9\n",
            "Successfully built minepy\n",
            "Installing collected packages: minepy\n",
            "Successfully installed minepy-1.2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install minepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from minepy import MINE\n",
        "\n",
        "mine = MINE()\n",
        "def pmic_feature_selection(F, C, m):\n",
        "    num_features = F.shape[1]\n",
        "    pmic_scores = np.zeros(num_features)\n",
        "\n",
        "    for i in range(num_features):\n",
        "        musk = ~np.isnan(F[:, i])\n",
        "        # Select column i without null values\n",
        "        feature_without_null = F[musk, i]\n",
        "\n",
        "        # Filter y based on non-null values in column i\n",
        "        class_without_null = C[musk]\n",
        "\n",
        "        # Calculate the MIC (Maximal Information Coefficient) score for the current feature and class\n",
        "        mine.compute_score(feature_without_null, class_without_null)\n",
        "        pmic_scores[i] = mine.mic()\n",
        "\n",
        "    # Sort the indices of pmic_scores in descending order and select the top m indices\n",
        "    top_m_features_idx = np.argsort(pmic_scores)[::-1][:m]\n",
        "    return top_m_features_idx"
      ],
      "metadata": {
        "id": "TGdIK_9NJzcS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Module 1 with synthetic data"
      ],
      "metadata": {
        "id": "drVZs9kNqD6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "num_class_dependent_features = 6\n",
        "num_class_independent_features = 3\n",
        "low = 0\n",
        "high = 1\n",
        "round = 2\n",
        "noise = 0.1\n",
        "seed = 42\n",
        "\n",
        "class_data, feature_data = generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed)\n",
        "\n",
        "selected_idx = pmic_feature_selection(feature_data, class_data, num_class_dependent_features)\n",
        "selected_features = feature_data[:, selected_idx]\n",
        "\n",
        "# print(\"\\nfeature_data: \\n\", feature_data)\n",
        "print(f\"\\nSelected {num_class_dependent_features} indices of features :\\n\", selected_idx)\n",
        "print(f\"\\nSelected {num_class_dependent_features} features :\\n\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omWdjpqAPVNl",
        "outputId": "d1390868-d49f-413d-8036-dbed6039541c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected 6 indices of features :\n",
            " [6 5 4 3 2 1]\n",
            "\n",
            "Selected 6 features :\n",
            " [[ 5.11  2.87  2.24  2.05  1.78  1.  ]\n",
            " [11.09  5.87  5.22  4.61  3.65  2.15]\n",
            " [ 9.75  5.25  4.5   3.68  2.44  2.37]\n",
            " ...\n",
            " [ 3.22  1.75  1.47  0.89  0.81  0.59]\n",
            " [11.29  6.05  5.24  4.62  3.38  2.19]\n",
            " [ 5.72  3.32  2.4   2.4   1.51  1.77]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 1 with Functions"
      ],
      "metadata": {
        "id": "KaGBfJPWrGVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 48\n",
        "percentage = 0.5\n",
        "select_top = 3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=0, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "# Generate a random 5x1 array with one-digit random values (0-9)\n",
        "Y = generate_random_array((row,), 0, 10, 0, seed=48, as_dataframe=False)\n",
        "print(\"\\ny: \\n\", Y)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "selected_idx = pmic_feature_selection(X_with_null, Y, select_top)\n",
        "selected_features = X_with_null[:, selected_idx]\n",
        "\n",
        "print(\"\\nX_with_null: \\n\", X_with_null)\n",
        "print(f\"\\nSelected {select_top} indices of features ({percentage * 100} percent null):\\n\", selected_idx)\n",
        "print(f\"\\nSelected {select_top} features ({percentage * 100} percent null):\\n\", selected_features)"
      ],
      "metadata": {
        "id": "A3gBqNfbih7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 : Imputation for the missing data"
      ],
      "metadata": {
        "id": "aqh3Unrhrexe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-negative Latent Factor\n",
        "\n",
        "Description:\n",
        "*   R: Incomplete data matrix of shape (n, m).\n",
        "*   d: Rank of the non-negative latent factors.\n",
        "*   lambda1, lambda2: Regularization parameters.\n",
        "*   max_iter: Maximum number of iterations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_QSlsNoFtwkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter):\n",
        "    # Initialize non-negative matrix P randomly\n",
        "    n, m = R.shape\n",
        "    R_copy = np.copy(R)\n",
        "    P = np.random.rand(n, d)\n",
        "\n",
        "    # Initialize non-negative matrix Q randomly\n",
        "    Q = np.random.rand(d, m)\n",
        "\n",
        "    # Initialize I according to (17)\n",
        "    I = np.ones((n, m))\n",
        "    R_nan_mask = np.isnan(R_copy)\n",
        "    I[R_nan_mask] = 0\n",
        "\n",
        "    # Set zero for Null\n",
        "    R_copy[R_nan_mask] = 0\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    iter = 0\n",
        "\n",
        "    # Convergence criterion\n",
        "    converge = False\n",
        "\n",
        "    while not converge and iter < max_iter:\n",
        "        # Update P according to (22)\n",
        "        P_new = P * ((I * R_copy) @ Q.T) / ((I * (P @ Q)) @ Q.T + lambda1 * P)\n",
        "\n",
        "        # Update Q according to (23)\n",
        "        Q_new = Q * (P_new.T @ (I * R_copy)) / (P_new.T @ (I * (P_new @ Q)) + lambda2 * Q)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.allclose(P, P_new) and np.allclose(Q, Q_new):\n",
        "            converge = True\n",
        "\n",
        "        # Update P and Q\n",
        "        P = P_new\n",
        "        Q = Q_new\n",
        "\n",
        "        # Increment iteration counter\n",
        "        iter += 1\n",
        "\n",
        "    # Impute R by (11) and obtain R_cpl\n",
        "    PQ = np.round( P @ Q, decimals=3)\n",
        "    R_cpl = np.where(R_nan_mask, PQ, R)\n",
        "\n",
        "    return R_cpl\n"
      ],
      "metadata": {
        "id": "gMXdVUfztlRl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperimpute"
      ],
      "metadata": {
        "id": "cbDeRI_665k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperimpute"
      ],
      "metadata": {
        "id": "JGuFnGw4664W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb686073-5dd7-40ed-affd-a7f03a3387bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hyperimpute in /usr/local/lib/python3.10/dist-packages (0.1.17)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (1.5.3)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (1.23.5)\n",
            "Requirement already satisfied: catboost>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (1.2.2)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (3.4.0)\n",
            "Requirement already satisfied: loguru==0.6.0 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (0.6.0)\n",
            "Requirement already satisfied: xgboost>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (2.0.1)\n",
            "Requirement already satisfied: miracle-imputation>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (0.1.5)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (4.1.0)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (5.0.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (2.2.1)\n",
            "Requirement already satisfied: geomloss in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (0.2.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (1.10.13)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (1.0.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from hyperimpute) (6.5.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost>=1.0.5->hyperimpute) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost>=1.0.5->hyperimpute) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost>=1.0.5->hyperimpute) (1.11.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost>=1.0.5->hyperimpute) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost>=1.0.5->hyperimpute) (1.16.0)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.10/dist-packages (from miracle-imputation>=0.1.3->hyperimpute) (2.14.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->hyperimpute) (1.12.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->hyperimpute) (6.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->hyperimpute) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->hyperimpute) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->hyperimpute) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->hyperimpute) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->hyperimpute) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->hyperimpute) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->hyperimpute) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->hyperimpute) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->hyperimpute) (2.1.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter->hyperimpute) (5.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->hyperimpute) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->hyperimpute) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->hyperimpute) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->hyperimpute) (7.7.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (6.3.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (23.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (5.9.2)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (0.18.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->hyperimpute) (1.0.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from redis->hyperimpute) (4.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->hyperimpute) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook->hyperimpute) (3.11.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->hyperimpute) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->hyperimpute) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->hyperimpute) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->hyperimpute) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->hyperimpute) (4.19.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->hyperimpute) (3.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.14.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->hyperimpute) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->hyperimpute) (21.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->hyperimpute) (7.34.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->hyperimpute) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->hyperimpute) (3.0.9)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->hyperimpute) (3.0.39)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost>=1.0.5->hyperimpute) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost>=1.0.5->hyperimpute) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost>=1.0.5->hyperimpute) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost>=1.0.5->hyperimpute) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost>=1.0.5->hyperimpute) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost>=1.0.5->hyperimpute) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost>=1.0.5->hyperimpute) (8.2.3)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter->hyperimpute) (2.4.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->hyperimpute) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.41.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (4.8.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->hyperimpute) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->hyperimpute) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->hyperimpute) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->hyperimpute) (0.12.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->hyperimpute) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->hyperimpute) (1.6.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->hyperimpute) (0.2.9)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.0.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->hyperimpute) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->hyperimpute) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->hyperimpute) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->hyperimpute) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->hyperimpute) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->hyperimpute) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->hyperimpute) (2.21)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->hyperimpute) (0.8.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.0->miracle-imputation>=0.1.3->hyperimpute) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hyperimpute.plugins.imputers import Imputers\n",
        "imputers = Imputers()\n",
        "\n",
        "X = pd.DataFrame([[1, 4, 7, 10], [4, 7, np.nan, np.nan], [3, 6, 9, 12], [8, 11, 14, 17]])\n",
        "\n",
        "method = \"gain\"\n",
        "\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(X.copy()).round(2)\n",
        "\n",
        "print(method, out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Dcu9uw7OUP",
        "outputId": "0f56cb67-a32f-420a-f4f2-ff174df3cac8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gain      0     1     2      3\n",
            "0  1.0   4.0   7.0  10.00\n",
            "1  4.0   7.0  10.3  13.45\n",
            "2  3.0   6.0   9.0  12.00\n",
            "3  8.0  11.0  14.0  17.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 2 Non-negative Latent Factor and GAIN with Functions"
      ],
      "metadata": {
        "id": "ETgB4s0O3RNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperimpute.plugins.imputers import Imputers\n",
        "\n",
        "seed = 48\n",
        "round = 2\n",
        "percentage = 0.3\n",
        "row = 5\n",
        "\n",
        "# Generate a random 5x4 array with one-digit random values (0-9)\n",
        "X = generate_random_array(shape=(row, 4), low=0, high=10, round=round, seed=seed, as_dataframe=False)\n",
        "print(\"complete X : \\n\", X)\n",
        "\n",
        "X_with_null = generate_random_nulls(dataset=X, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "print(\"\\nX_with_null : \\n\", X_with_null)\n",
        "\n",
        "R = np.copy(X_with_null)\n",
        "d = 2\n",
        "lambda1 = 0.1\n",
        "lambda2 = 0.2\n",
        "max_iter = 100\n",
        "\n",
        "# Code execution\n",
        "R_cpl = Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter)\n",
        "print(\"\\nComplete data after imputation by non_negative_latent_factor: \\n\", R_cpl)\n",
        "\n",
        "\n",
        "method = \"gain\"\n",
        "plugin = Imputers().get(method)\n",
        "out = plugin.fit_transform(R.copy()).round(round)\n",
        "\n",
        "print(f'\\nComplete data after imputation by {method}: \\n', out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9A-zBe5Lbs",
        "outputId": "75d277ee-0f59-4509-f72b-1fb23dc7176f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete X : \n",
            " [[0.17 8.92 2.85 2.99]\n",
            " [7.92 3.24 8.65 4.48]\n",
            " [5.48 3.57 1.12 1.42]\n",
            " [4.45 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "X_with_null : \n",
            " [[0.17  nan 2.85 2.99]\n",
            " [7.92  nan 8.65  nan]\n",
            " [ nan  nan 1.12 1.42]\n",
            " [ nan 7.32 4.6  5.93]\n",
            " [3.37 4.54 1.87 4.09]]\n",
            "\n",
            "Complete data after imputation by non_negative_latent_factor: \n",
            " [[ 0.17   0.235  2.85   2.99 ]\n",
            " [ 7.92   9.428  8.65  10.738]\n",
            " [ 0.906  1.078  1.12   1.42 ]\n",
            " [ 5.593  7.32   4.6    5.93 ]\n",
            " [ 3.37   4.54   1.87   4.09 ]]\n",
            "\n",
            "Complete data after imputation by gain: \n",
            "       0     1     2     3\n",
            "0  0.17  4.57  2.85  2.99\n",
            "1  7.92  5.84  8.65  4.05\n",
            "2  1.29  4.57  1.12  1.42\n",
            "3  3.75  7.32  4.60  5.93\n",
            "4  3.37  4.54  1.87  4.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Module 2 with synthetic data"
      ],
      "metadata": {
        "id": "6rsWIbH7vPeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fancyimpute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78d_s3SBHDu",
        "outputId": "182b9727-51f1-4a9e-bc1a-4fadd966aad6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fancyimpute in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: knnimpute>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (0.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.2.2)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (7.4.3)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from fancyimpute) (1.3.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (3.2.0)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (3.2.4)\n",
            "Requirement already satisfied: setuptools>65.5.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy->fancyimpute) (67.7.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->fancyimpute) (2.0.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.7.post0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperimpute.plugins.imputers import Imputers\n",
        "imputers = Imputers()\n",
        "\n",
        "imputers.list()\n",
        "\n",
        "num_samples = 80\n",
        "num_class_dependent_features = 100\n",
        "num_class_independent_features = 50\n",
        "low = 0\n",
        "high = 1\n",
        "round = 2\n",
        "noise = 0.1\n",
        "seed = 42\n",
        "percentage = 0.3\n",
        "\n",
        "class_data, feature_data = generate_synthetic_data(num_samples, num_class_dependent_features, num_class_independent_features, noise, low, high, round, seed)\n",
        "# print(\"\\nFeature_data : \\n\", feature_data)\n",
        "feature_data_with_null = generate_random_nulls(dataset=feature_data, percentage=percentage, seed=seed, as_dataframe=False)\n",
        "# print(\"\\nfeature_data_with_null : \\n\", feature_data_with_null)\n",
        "\n",
        "# selected_idx = pmic_feature_selection(feature_data, class_data, num_class_dependent_features)\n",
        "# selected_features = feature_data[:, selected_idx]\n",
        "R = np.copy(feature_data_with_null)\n",
        "d = 2\n",
        "lambda1 = 0.1\n",
        "lambda2 = 0.2\n",
        "max_iter = 100\n",
        "\n",
        "# Code execution\n",
        "R_cpl = Imputes_the_missing_values_By_non_negative_latent_factor(R, d, lambda1, lambda2, max_iter)\n",
        "# print(\"\\nComplete data after imputation by non_negative_latent_factor: \\n\", R_cpl)\n",
        "\n",
        "\n",
        "method = \"gain\"\n",
        "plugin = Imputers().get(method)\n",
        "gain_out = plugin.fit_transform(R.copy()).round(round)\n",
        "\n",
        "# method = \"miwae\"\n",
        "# plugin = Imputers().get(\"miwae\")\n",
        "# miwae_out = plugin.fit_transform(R.copy())\n",
        "\n",
        "\n",
        "# print(f'\\nComplete data after imputation by {method}: \\n', gain_out)\n",
        "\n",
        "# Implement MICE using fancyimpute\n",
        "mice_imputer = IterativeImputer()\n",
        "X_imputed_mice = mice_imputer.fit_transform(feature_data_with_null)\n",
        "\n",
        "\n",
        "# Error calculation:\n",
        "\n",
        "# For all these error metrics, smaller values are generally considered better,\n",
        "# except for the R-squared metric where higher values indicate a better fit.\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Mean Absolute Error (MAE): It is the average absolute difference between corresponding elements of the two matrices.\n",
        "mae_non_negative_latent_factor = mean_absolute_error(feature_data, R_cpl)\n",
        "print(\"\\n mae error for non_negative_latent_factor method : \",mae_non_negative_latent_factor)\n",
        "mae_mice = mean_absolute_error(feature_data, X_imputed_mice)\n",
        "print(\"\\n mae error for mice method : \",mae_mice)\n",
        "mae_gain = mean_absolute_error(feature_data, gain_out)\n",
        "print(\"\\n mae error for gain method : \",mae_gain)\n",
        "\n",
        "# Mean Squared Error (MSE): It is the average of the squared differences between corresponding elements of the two matrices.\n",
        "mse_non_negative_latent_factor = mean_squared_error(feature_data.flatten(), R_cpl.flatten())\n",
        "print(\"\\n mse error for non_negative_latent_factor method : \",mse_non_negative_latent_factor)\n",
        "mse_mice = mean_squared_error(feature_data.flatten(), X_imputed_mice.flatten())\n",
        "print(\"\\n mse error for mice method : \",mse_mice)\n",
        "mse_gain = mean_squared_error(feature_data.flatten(), gain_out.to_numpy().flatten())\n",
        "print(\"\\n mse error for gain method : \",mse_gain)\n",
        "\n",
        "# Root Mean Squared Error (RMSE): It is the square root of the MSE.\n",
        "rmse_non_negative_latent_factor = math.sqrt(mse_non_negative_latent_factor)\n",
        "print(\"\\n rmse error for non_negative_latent_factor method : \",rmse_non_negative_latent_factor)\n",
        "rmse_mice = math.sqrt(mse_mice)\n",
        "print(\"\\n rmse error for mice method : \",rmse_mice)\n",
        "rmse_gain = math.sqrt(mse_gain)\n",
        "print(\"\\n rmse error for gain method : \",rmse_gain)\n",
        "\n",
        "# R-squared (Coefficient of Determination): It is a statistical measure that indicates the proportion of the variance in the dependent variable\n",
        "# that is predictable from the independent variable(s).\n",
        "r2_non_negative_latent_factor = r2_score(feature_data.flatten(), R_cpl.flatten())\n",
        "print(\"\\n r2 error for non_negative_latent_factor method : \",r2_non_negative_latent_factor)\n",
        "r2_mice = r2_score(feature_data.flatten(), X_imputed_mice.flatten())\n",
        "print(\"\\n r2 error for mice method : \",r2_mice)\n",
        "r2_gain = r2_score(feature_data.flatten(), gain_out.to_numpy().flatten())\n",
        "print(\"\\n r2 error for gain method : \",r2_gain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USo5agSyq_uR",
        "outputId": "1195f9b0-e1ca-4b2e-dfc7-d566e98a4efa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " mae error for non_negative_latent_factor method :  0.08754495033112583\n",
            "\n",
            " mae error for mice method :  0.09500944776527581\n",
            "\n",
            " mae error for gain method :  1.6162408940397355\n",
            "\n",
            " mse error for non_negative_latent_factor method :  0.03764798220198675\n",
            "\n",
            " mse error for mice method :  0.0465600876802904\n",
            "\n",
            " mse error for gain method :  34.08456911423841\n",
            "\n",
            " rmse error for non_negative_latent_factor method :  0.194030879506296\n",
            "\n",
            " rmse error for mice method :  0.21577786652085149\n",
            "\n",
            " rmse error for gain method :  5.838199132801005\n",
            "\n",
            " r2 error for non_negative_latent_factor method :  0.9999265755721893\n",
            "\n",
            " r2 error for mice method :  0.99990919439511\n",
            "\n",
            " r2 error for gain method :  0.9335252558567604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3 : Select Featurs"
      ],
      "metadata": {
        "id": "lb7eMgONam2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Dimension Reduction and Feature Extraction\n"
      ],
      "metadata": {
        "id": "slGFvUqH59kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Encoder Layer Class\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "# Autoencoder Class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes=None):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = EncoderLayer(input_size, hidden_size)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes:\n",
        "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        if self.num_classes:\n",
        "            logits = self.classifier(encoded)\n",
        "            return decoded, logits\n",
        "        else:\n",
        "            return decoded\n",
        "\n",
        "# Function to generate random array data\n",
        "def generate_random_array(shape, low, high, round_val, seed=None, as_dataframe=False):\n",
        "    np.random.seed(seed)\n",
        "    random_array = np.random.uniform(low, high, size=shape).round(round_val)\n",
        "    if as_dataframe:\n",
        "        return pd.DataFrame(random_array)\n",
        "    return random_array\n",
        "\n",
        "# Function to generate random null values in the dataset\n",
        "def generate_random_nulls(dataset, percentage, seed=None, as_dataframe=False):\n",
        "    temp = dataset.copy()\n",
        "    np.random.seed(seed)\n",
        "    null_mask_indices = np.random.choice(range(temp.size), size=int(temp.size * percentage), replace=False)\n",
        "    if as_dataframe:\n",
        "        df_null_mask = pd.DataFrame(False, index=temp.index, columns=temp.columns)\n",
        "        df_null_mask.values.flat[null_mask_indices] = True\n",
        "        df_masked = temp.where(~df_null_mask)\n",
        "        return df_masked\n",
        "    temp.ravel()[null_mask_indices] = np.nan\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "Pf2mTxSx6H6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 1: Unsupervised Learning"
      ],
      "metadata": {
        "id": "FyW9tTBr29-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 1: Unsupervised Learning\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=1)\n",
        "\n",
        "# Create an instance of the Autoencoder model for Scenario 1\n",
        "autoencoder_unsupervised = Autoencoder(input_size=10, hidden_size=5)\n",
        "\n",
        "# Train the Autoencoder model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_unsupervised.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    outputs = autoencoder_unsupervised(inputs)\n",
        "    loss = criterion(outputs, inputs)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_unsupervised.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "XBCkOrBz3DYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 2: Supervised Learning with 1 Class"
      ],
      "metadata": {
        "id": "eQdw_d183F_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 2: Supervised Learning with 1 Class\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=2)\n",
        "labels = np.random.randint(0, 2, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 2\n",
        "autoencoder_supervised_1class = Autoencoder(input_size=10, hidden_size=5, num_classes=1)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_1class.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_1class(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_1class.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "F05ieObr3JTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z8ibCPrGV38u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenario 3: Supervised Learning with Multiple Classes (3)"
      ],
      "metadata": {
        "id": "PbQAuBjO3Len"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 3: Supervised Learning with Multiple Classes (3)\n",
        "\n",
        "# Generate sample data\n",
        "data = generate_random_array((100, 10), 0, 100, 2, seed=3)\n",
        "labels = np.random.randint(0, 3, size=(100,))\n",
        "\n",
        "# Create an instance of the Autoencoder with Classification model for Scenario 3\n",
        "autoencoder_supervised_multiclass = Autoencoder(input_size=10, hidden_size=5, num_classes=3)\n",
        "\n",
        "# Train the Autoencoder with Classification model\n",
        "criterion = nn.MSELoss()\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(autoencoder_supervised_multiclass.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(100):\n",
        "    inputs = torch.Tensor(data)\n",
        "    labels_tensor = torch.Tensor(labels).long()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, logits = autoencoder_supervised_multiclass(inputs)\n",
        "    reconstruction_loss = criterion(outputs, inputs)\n",
        "    classification_loss = classification_criterion(logits.squeeze(), labels_tensor)\n",
        "\n",
        "    # Total loss\n",
        "    loss = reconstruction_loss + classification_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Encode the entire dataset\n",
        "encoded_data = autoencoder_supervised_multiclass.encoder(torch.Tensor(data)).detach().numpy()\n",
        "\n",
        "print(\"Encoded Data:\")\n",
        "print(encoded_data)\n",
        "\n",
        "# Normalize the encoded data between 0 and 1 for each column\n",
        "normalized_data = np.zeros_like(encoded_data)\n",
        "\n",
        "for i in range(encoded_data.shape[1]):\n",
        "    column = encoded_data[:, i]\n",
        "    min_val = np.min(column)\n",
        "    max_val = np.max(column)\n",
        "    if max_val - min_val == 0:\n",
        "        normalized_column = np.zeros_like(column)\n",
        "    else:\n",
        "        normalized_column = (column - min_val) / (max_val - min_val)\n",
        "    normalized_column = np.round(normalized_column, decimals=4)  # Round to 4 decimal places\n",
        "    normalized_data[:, i] = normalized_column\n",
        "\n",
        "print(\"Normalized Encoded Data:\")\n",
        "print(normalized_data)"
      ],
      "metadata": {
        "id": "09S0N6PP3PNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Feature Screening\n",
        "\n",
        "Screen all the features via multivariate rank distance correlation learning to select the relevant ones.\n",
        "\n",
        "The reason for using feature screening instead of neural network is the small amount of training samples.\n",
        "\n",
        "When training samples are limited, neural network can easily result in overfitting; however, if we only consider one feature at a time, the dependence between the feature and xencoded can still be well estimated even when the sample size is small."
      ],
      "metadata": {
        "id": "-bv_ikz3Ra3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def multivariate_rank_distance(X, Y=None):\n",
        "    \"\"\"\n",
        "    Compute the multivariate rank distance correlation (MrDc) between two sets of variables X and Y.\n",
        "    If Y is not provided, it computes the unsupervised MrDc.\n",
        "    Args:\n",
        "        X (ndarray): Predictor variables, shape (n, p).\n",
        "        Y (ndarray): Response variables, shape (n, m).\n",
        "    Returns:\n",
        "        float: The MrDc value.\n",
        "    \"\"\"\n",
        "    n, p = X.shape\n",
        "\n",
        "    if Y is None:  # Unsupervised MrDc\n",
        "        Y = X\n",
        "\n",
        "    # Compute pairwise distance matrices\n",
        "    X_dist = squareform(pdist(X, 'euclidean'))\n",
        "    Y_dist = squareform(pdist(Y, 'euclidean'))\n",
        "\n",
        "    # Compute ranks\n",
        "    X_ranks = np.argsort(X_dist, axis=0).argsort(axis=0)\n",
        "    Y_ranks = np.argsort(Y_dist, axis=0).argsort(axis=0)\n",
        "\n",
        "    # Compute rank distance correlation\n",
        "    R = np.sum((X_ranks - Y_ranks) ** 2) / (n * (n - 1))\n",
        "    return np.sqrt(R)\n",
        "\n",
        "def mrdc_sis(X, k, y=None):\n",
        "    \"\"\"\n",
        "    Perform feature screening using the MrDc-SIS method.\n",
        "    Args:\n",
        "        X (ndarray): Predictor variables, shape (n, p).\n",
        "        k (int): Number of top-ranked variables to select.\n",
        "        y (ndarray): Response variable (optional), shape (n,).\n",
        "    Returns:\n",
        "        ndarray: Indices of the selected variables.\n",
        "    \"\"\"\n",
        "    p = X.shape[1]\n",
        "    scores = np.zeros(p)\n",
        "\n",
        "    # Compute MrDc scores for each predictor variable\n",
        "    for i in range(p):\n",
        "        if y is not None:  # Supervised MrDc\n",
        "            scores[i] = multivariate_rank_distance(X[:, i:i+1], y.reshape(-1, 1))\n",
        "        else:  # Unsupervised MrDc\n",
        "            scores[i] = multivariate_rank_distance(X[:, i:i+1])\n",
        "\n",
        "    # Rank the variables based on their scores\n",
        "    ranks = np.argsort(scores)\n",
        "    selected_indices = ranks[:k]\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "# Example usage for both supervised and unsupervised feature screening\n",
        "# Generate random data\n",
        "np.random.seed(0)\n",
        "n = 100  # Sample size\n",
        "p = 10   # Number of predictor variables\n",
        "m = 3    # Number of response variables\n",
        "X = np.random.rand(n, p)\n",
        "y = np.random.rand(n)\n",
        "\n",
        "# Perform supervised feature screening with MrDc-SIS\n",
        "k = 3  # Number of top-ranked variables to select\n",
        "selected_indices_supervised = mrdc_sis(X, k, y)\n",
        "selected_variables_supervised = X[:, selected_indices_supervised]\n",
        "\n",
        "print(\"Selected variables indices (supervised):\", selected_indices_supervised)\n",
        "print(\"Selected variables (supervised):\")\n",
        "print(selected_variables_supervised)\n",
        "\n",
        "# Perform unsupervised feature screening with MrDc-SIS\n",
        "selected_indices_unsupervised = mrdc_sis(X, k)\n",
        "selected_variables_unsupervised = X[:, selected_indices_unsupervised]\n",
        "\n",
        "print(\"Selected variables indices (unsupervised):\", selected_indices_unsupervised)\n",
        "print(\"Selected variables (unsupervised):\")\n",
        "print(selected_variables_unsupervised)"
      ],
      "metadata": {
        "id": "vbcctKR3kYXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# Generate a random dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(5, 3)\n",
        "\n",
        "# Compute pairwise distances using Euclidean distance metric\n",
        "distances = pdist(X, metric='euclidean')\n",
        "\n",
        "print(\"X : \", X)\n",
        "\n",
        "print(\"Pairwise distances:\")\n",
        "print(distances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dql3I5E7nxpi",
        "outputId": "09f26f57-67bb-46ae-94bf-280fc9c44b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X :  [[0.5488135  0.71518937 0.60276338]\n",
            " [0.54488318 0.4236548  0.64589411]\n",
            " [0.43758721 0.891773   0.96366276]\n",
            " [0.38344152 0.79172504 0.52889492]\n",
            " [0.56804456 0.92559664 0.07103606]]\n",
            "Pairwise distances:\n",
            "[0.29473397 0.41689499 0.19662693 0.57216693 0.57586803 0.41860234\n",
            " 0.76350759 0.44940452 0.90274337 0.51150232]\n"
          ]
        }
      ]
    }
  ]
}